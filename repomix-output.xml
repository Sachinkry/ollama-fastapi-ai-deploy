This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    ci.yml
app/
  api/
    __init__.py
    routes_generate.py
    routes_models.py
    routes_status.py
  core/
    auth.py
    celery_app.py
    config.py
    logger.py
    metrics.py
  models/
    llm_model.py
    model_registry.py
  schemas/
    inference.py
  services/
    ollama_client.py
  static/
    index.html
  utils/
    inference_utils.py
  main.py
  worker.py
frontend/
  public/
    vite.svg
  src/
    assets/
      react.svg
    components/
      ui/
        button.tsx
        card.tsx
        label.tsx
        select.tsx
        slider.tsx
        textarea.tsx
    lib/
      utils.ts
    App.css
    App.tsx
    index.css
    main.tsx
  .gitignore
  components.json
  Dockerfile.frontend
  eslint.config.js
  index.html
  nginx.conf
  package.json
  README.md
  tsconfig.app.json
  tsconfig.json
  tsconfig.node.json
  vite.config.ts
tests/
  test_api_generate.py
  test_api_models.py
  test_main.py
.env.example
.gitignore
ai-file.xml
docker-compose.yml
Dockerfile
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="ai-file.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    ci.yml
app/
  api/
    __init__.py
    routes_generate.py
    routes_models.py
    routes_status.py
  core/
    auth.py
    celery_app.py
    config.py
    logger.py
    metrics.py
  models/
    llm_model.py
    model_registry.py
  schemas/
    inference.py
  services/
    ollama_client.py
  static/
    index.html
  utils/
    inference_utils.py
  main.py
  worker.py
frontend/
  public/
    vite.svg
  src/
    assets/
      react.svg
    components/
      ui/
        button.tsx
        card.tsx
        label.tsx
        select.tsx
        slider.tsx
        textarea.tsx
    lib/
      utils.ts
    App.css
    App.tsx
    index.css
    main.tsx
  .gitignore
  components.json
  Dockerfile.frontend
  eslint.config.js
  index.html
  nginx.conf
  package.json
  README.md
  tsconfig.app.json
  tsconfig.json
  tsconfig.node.json
  vite.config.ts
tests/
  test_api_generate.py
  test_api_models.py
  test_main.py
.env.example
.gitignore
docker-compose.yml
Dockerfile
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/core/auth.py">
from fastapi import Header, HTTPException, status
from app.core.config import settings

def require_api_key(x_api_key: str | None = Header(None)):
    if not x_api_key or x_api_key not in settings.API_KEYS:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Missing or invalid API key"
        )
</file>

<file path="app/core/config.py">
import os

class Settings:
    OLLAMA_HOST: str = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    # Control-plane limits
    CONCURRENCY_LIMIT: int = int(os.getenv("CONCURRENCY_LIMIT", "4"))
    REQUEST_TIMEOUT_S: float = float(os.getenv("REQUEST_TIMEOUT_S", "60"))
    # super basic API keys (move to Redis/DB later)
    API_KEYS: set[str] = set(os.getenv("API_KEYS", "dev-key-123").split(","))

settings = Settings()
</file>

<file path="app/core/logger.py">
import logging, sys

def setup_logger():
    log = logging.getLogger("prodify")
    log.setLevel(logging.INFO)
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(logging.Formatter(
        fmt="%(asctime)s | %(levelname)s | %(message)s"
    ))
    log.handlers = [handler]
    return log

logger = setup_logger()
</file>

<file path="app/core/metrics.py">
from prometheus_client import Counter, Histogram, CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST

registry = CollectorRegistry()
REQUESTS = Counter("requests_total", "Total requests", ["route","model"], registry=registry)
ERRORS   = Counter("errors_total", "Errors", ["route","model"], registry=registry)
LATENCY  = Histogram("latency_seconds", "Request latency", ["route","model"], registry=registry)

def render_metrics():
    return generate_latest(registry), CONTENT_TYPE_LATEST
</file>

<file path="app/models/llm_model.py">
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class LocalLLM:
    def __init__(self, model_name="distilgpt2"):
        print(f"Loading model: {model_name}")
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.model.to(self.device)

    def generate(self, prompt: str, max_new_tokens: int = 80):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.8,
            top_p=0.95,
        )
        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return text.strip()
</file>

<file path="app/models/model_registry.py">
# app/models/model_registry.py
from app.models.llm_model import LocalLLM
import threading

class ModelRegistry:
    def __init__(self):
        self.models = {}
        self.initialized = False
        print("üöÄ ModelRegistry initialized but models not loaded yet.")

    def _load_models(self):
        print("üß† Loading models in background thread...")
        try:
            self.models = {
                "distilgpt2": LocalLLM("distilgpt2"),
                "DialoGPT-small": LocalLLM("microsoft/DialoGPT-small"),
                # Comment out TinyLlama until stable (too big for M1)
                # "TinyLlama": LocalLLM("TinyLlama/TinyLlama-1.1B-Chat-v0.6"),
            }
            self.initialized = True
            print(f"‚úÖ Models ready: {list(self.models.keys())}")
        except Exception as e:
            print(f"‚ùå Error loading models: {e}")

    def list_models(self):
        if not self.initialized:
            return []
        return list(self.models.keys())

    def get_model(self, name: str):
        if not self.initialized:
            raise RuntimeError("Models are still loading. Try again later.")
        if name not in self.models:
            raise ValueError(f"Model '{name}' not found.")
        return self.models[name]

    def start_loading(self):
        threading.Thread(target=self._load_models, daemon=True).start()


# Singleton instance
model_registry = ModelRegistry()
</file>

<file path="app/schemas/inference.py">
# app/schemas/inference.py
from pydantic import BaseModel

class InferenceRequest(BaseModel):
    model_name: str ="sentiment"
    input_text: str

    class Config:
        json_schema_extra = {
            "example": {
                "model_name": "sentiment",
                "input_text": "The movie was fantastic!"
            }
        }

class InferenceResponse(BaseModel):
    label: str
    score: float
</file>

<file path="app/static/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Prodify Model API</title>
  <style>
    body { font-family: Inter, sans-serif; background:#fafafa; margin: 40px; }
    .container { max-width: 700px; margin:auto; }
    textarea, select, input { width:100%; padding:8px; margin-top:8px; border-radius:6px; border:1px solid #ccc; }
    button { margin-top:10px; padding:10px 18px; border:none; background:#0077ff; color:white; border-radius:6px; cursor:pointer; }
    button:hover { background:#0059c9; }
    pre { background:white; padding:15px; border-radius:8px; box-shadow:0 0 6px rgba(0,0,0,0.1); white-space:pre-wrap; }
    .loading { animation: blink 1s infinite; }
    @keyframes blink { 50% { opacity: 0.5; } }
  </style>
</head>
<body>
  <div class="container">
    <h1>üöÄ Prodify Inference</h1>
    <label>Model:</label>
    <select id="model"></select>
    <label>Prompt:</label>
    <textarea id="prompt">Once upon a time in Bangalore,</textarea>
    <button onclick="generate()">Generate</button>
    <pre id="output"></pre>
  </div>

  <script>
    async function loadModels() {
      const res = await fetch("/generate/models");
      const data = await res.json();
      const select = document.getElementById("model");
      data.available_models.forEach(m => {
        const opt = document.createElement("option");
        opt.value = m;
        opt.textContent = m;
        select.appendChild(opt);
      });
    }

    async function generate() {
      const model = document.getElementById("model").value;
      const prompt = document.getElementById("prompt").value;
      const output = document.getElementById("output");
      output.textContent = "‚è≥ Generating...";
      output.classList.add("loading");

      const res = await fetch("/generate/", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ model, prompt, max_tokens: 80 })
      });

      const data = await res.json();
      output.classList.remove("loading");
      output.textContent = data.generated_text || JSON.stringify(data, null, 2);
    }

    loadModels();
  </script>
</body>
</html>
</file>

<file path="frontend/public/vite.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>
</file>

<file path="frontend/src/assets/react.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="35.93" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 228"><path fill="#00D8FF" d="M210.483 73.824a171.49 171.49 0 0 0-8.24-2.597c.465-1.9.893-3.777 1.273-5.621c6.238-30.281 2.16-54.676-11.769-62.708c-13.355-7.7-35.196.329-57.254 19.526a171.23 171.23 0 0 0-6.375 5.848a155.866 155.866 0 0 0-4.241-3.917C100.759 3.829 77.587-4.822 63.673 3.233C50.33 10.957 46.379 33.89 51.995 62.588a170.974 170.974 0 0 0 1.892 8.48c-3.28.932-6.445 1.924-9.474 2.98C17.309 83.498 0 98.307 0 113.668c0 15.865 18.582 31.778 46.812 41.427a145.52 145.52 0 0 0 6.921 2.165a167.467 167.467 0 0 0-2.01 9.138c-5.354 28.2-1.173 50.591 12.134 58.266c13.744 7.926 36.812-.22 59.273-19.855a145.567 145.567 0 0 0 5.342-4.923a168.064 168.064 0 0 0 6.92 6.314c21.758 18.722 43.246 26.282 56.54 18.586c13.731-7.949 18.194-32.003 12.4-61.268a145.016 145.016 0 0 0-1.535-6.842c1.62-.48 3.21-.974 4.76-1.488c29.348-9.723 48.443-25.443 48.443-41.52c0-15.417-17.868-30.326-45.517-39.844Zm-6.365 70.984c-1.4.463-2.836.91-4.3 1.345c-3.24-10.257-7.612-21.163-12.963-32.432c5.106-11 9.31-21.767 12.459-31.957c2.619.758 5.16 1.557 7.61 2.4c23.69 8.156 38.14 20.213 38.14 29.504c0 9.896-15.606 22.743-40.946 31.14Zm-10.514 20.834c2.562 12.94 2.927 24.64 1.23 33.787c-1.524 8.219-4.59 13.698-8.382 15.893c-8.067 4.67-25.32-1.4-43.927-17.412a156.726 156.726 0 0 1-6.437-5.87c7.214-7.889 14.423-17.06 21.459-27.246c12.376-1.098 24.068-2.894 34.671-5.345a134.17 134.17 0 0 1 1.386 6.193ZM87.276 214.515c-7.882 2.783-14.16 2.863-17.955.675c-8.075-4.657-11.432-22.636-6.853-46.752a156.923 156.923 0 0 1 1.869-8.499c10.486 2.32 22.093 3.988 34.498 4.994c7.084 9.967 14.501 19.128 21.976 27.15a134.668 134.668 0 0 1-4.877 4.492c-9.933 8.682-19.886 14.842-28.658 17.94ZM50.35 144.747c-12.483-4.267-22.792-9.812-29.858-15.863c-6.35-5.437-9.555-10.836-9.555-15.216c0-9.322 13.897-21.212 37.076-29.293c2.813-.98 5.757-1.905 8.812-2.773c3.204 10.42 7.406 21.315 12.477 32.332c-5.137 11.18-9.399 22.249-12.634 32.792a134.718 134.718 0 0 1-6.318-1.979Zm12.378-84.26c-4.811-24.587-1.616-43.134 6.425-47.789c8.564-4.958 27.502 2.111 47.463 19.835a144.318 144.318 0 0 1 3.841 3.545c-7.438 7.987-14.787 17.08-21.808 26.988c-12.04 1.116-23.565 2.908-34.161 5.309a160.342 160.342 0 0 1-1.76-7.887Zm110.427 27.268a347.8 347.8 0 0 0-7.785-12.803c8.168 1.033 15.994 2.404 23.343 4.08c-2.206 7.072-4.956 14.465-8.193 22.045a381.151 381.151 0 0 0-7.365-13.322Zm-45.032-43.861c5.044 5.465 10.096 11.566 15.065 18.186a322.04 322.04 0 0 0-30.257-.006c4.974-6.559 10.069-12.652 15.192-18.18ZM82.802 87.83a323.167 323.167 0 0 0-7.227 13.238c-3.184-7.553-5.909-14.98-8.134-22.152c7.304-1.634 15.093-2.97 23.209-3.984a321.524 321.524 0 0 0-7.848 12.897Zm8.081 65.352c-8.385-.936-16.291-2.203-23.593-3.793c2.26-7.3 5.045-14.885 8.298-22.6a321.187 321.187 0 0 0 7.257 13.246c2.594 4.48 5.28 8.868 8.038 13.147Zm37.542 31.03c-5.184-5.592-10.354-11.779-15.403-18.433c4.902.192 9.899.29 14.978.29c5.218 0 10.376-.117 15.453-.343c-4.985 6.774-10.018 12.97-15.028 18.486Zm52.198-57.817c3.422 7.8 6.306 15.345 8.596 22.52c-7.422 1.694-15.436 3.058-23.88 4.071a382.417 382.417 0 0 0 7.859-13.026a347.403 347.403 0 0 0 7.425-13.565Zm-16.898 8.101a358.557 358.557 0 0 1-12.281 19.815a329.4 329.4 0 0 1-23.444.823c-7.967 0-15.716-.248-23.178-.732a310.202 310.202 0 0 1-12.513-19.846h.001a307.41 307.41 0 0 1-10.923-20.627a310.278 310.278 0 0 1 10.89-20.637l-.001.001a307.318 307.318 0 0 1 12.413-19.761c7.613-.576 15.42-.876 23.31-.876H128c7.926 0 15.743.303 23.354.883a329.357 329.357 0 0 1 12.335 19.695a358.489 358.489 0 0 1 11.036 20.54a329.472 329.472 0 0 1-11 20.722Zm22.56-122.124c8.572 4.944 11.906 24.881 6.52 51.026c-.344 1.668-.73 3.367-1.15 5.09c-10.622-2.452-22.155-4.275-34.23-5.408c-7.034-10.017-14.323-19.124-21.64-27.008a160.789 160.789 0 0 1 5.888-5.4c18.9-16.447 36.564-22.941 44.612-18.3ZM128 90.808c12.625 0 22.86 10.235 22.86 22.86s-10.235 22.86-22.86 22.86s-22.86-10.235-22.86-22.86s10.235-22.86 22.86-22.86Z"></path></svg>
</file>

<file path="frontend/src/components/ui/button.tsx">
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const buttonVariants = cva(
  "inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-white hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost:
          "hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2 has-[>svg]:px-3",
        sm: "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
        lg: "h-10 rounded-md px-6 has-[>svg]:px-4",
        icon: "size-9",
        "icon-sm": "size-8",
        "icon-lg": "size-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)

function Button({
  className,
  variant,
  size,
  asChild = false,
  ...props
}: React.ComponentProps<"button"> &
  VariantProps<typeof buttonVariants> & {
    asChild?: boolean
  }) {
  const Comp = asChild ? Slot : "button"

  return (
    <Comp
      data-slot="button"
      className={cn(buttonVariants({ variant, size, className }))}
      {...props}
    />
  )
}

export { Button, buttonVariants }
</file>

<file path="frontend/src/components/ui/card.tsx">
import * as React from "react"

import { cn } from "@/lib/utils"

function Card({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card"
      className={cn(
        "bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm",
        className
      )}
      {...props}
    />
  )
}

function CardHeader({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-header"
      className={cn(
        "@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6",
        className
      )}
      {...props}
    />
  )
}

function CardTitle({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-title"
      className={cn("leading-none font-semibold", className)}
      {...props}
    />
  )
}

function CardDescription({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

function CardAction({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-action"
      className={cn(
        "col-start-2 row-span-2 row-start-1 self-start justify-self-end",
        className
      )}
      {...props}
    />
  )
}

function CardContent({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-content"
      className={cn("px-6", className)}
      {...props}
    />
  )
}

function CardFooter({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-footer"
      className={cn("flex items-center px-6 [.border-t]:pt-6", className)}
      {...props}
    />
  )
}

export {
  Card,
  CardHeader,
  CardFooter,
  CardTitle,
  CardAction,
  CardDescription,
  CardContent,
}
</file>

<file path="frontend/src/components/ui/label.tsx">
"use client"

import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"

import { cn } from "@/lib/utils"

function Label({
  className,
  ...props
}: React.ComponentProps<typeof LabelPrimitive.Root>) {
  return (
    <LabelPrimitive.Root
      data-slot="label"
      className={cn(
        "flex items-center gap-2 text-sm leading-none font-medium select-none group-data-[disabled=true]:pointer-events-none group-data-[disabled=true]:opacity-50 peer-disabled:cursor-not-allowed peer-disabled:opacity-50",
        className
      )}
      {...props}
    />
  )
}

export { Label }
</file>

<file path="frontend/src/components/ui/select.tsx">
import * as React from "react"
import * as SelectPrimitive from "@radix-ui/react-select"
import { CheckIcon, ChevronDownIcon, ChevronUpIcon } from "lucide-react"

import { cn } from "@/lib/utils"

function Select({
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Root>) {
  return <SelectPrimitive.Root data-slot="select" {...props} />
}

function SelectGroup({
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Group>) {
  return <SelectPrimitive.Group data-slot="select-group" {...props} />
}

function SelectValue({
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Value>) {
  return <SelectPrimitive.Value data-slot="select-value" {...props} />
}

function SelectTrigger({
  className,
  size = "default",
  children,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Trigger> & {
  size?: "sm" | "default"
}) {
  return (
    <SelectPrimitive.Trigger
      data-slot="select-trigger"
      data-size={size}
      className={cn(
        "border-input data-[placeholder]:text-muted-foreground [&_svg:not([class*='text-'])]:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 dark:hover:bg-input/50 flex w-fit items-center justify-between gap-2 rounded-md border bg-transparent px-3 py-2 text-sm whitespace-nowrap shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 data-[size=default]:h-9 data-[size=sm]:h-8 *:data-[slot=select-value]:line-clamp-1 *:data-[slot=select-value]:flex *:data-[slot=select-value]:items-center *:data-[slot=select-value]:gap-2 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
        className
      )}
      {...props}
    >
      {children}
      <SelectPrimitive.Icon asChild>
        <ChevronDownIcon className="size-4 opacity-50" />
      </SelectPrimitive.Icon>
    </SelectPrimitive.Trigger>
  )
}

function SelectContent({
  className,
  children,
  position = "popper",
  align = "center",
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Content>) {
  return (
    <SelectPrimitive.Portal>
      <SelectPrimitive.Content
        data-slot="select-content"
        className={cn(
          "bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 relative z-50 max-h-(--radix-select-content-available-height) min-w-[8rem] origin-(--radix-select-content-transform-origin) overflow-x-hidden overflow-y-auto rounded-md border shadow-md",
          position === "popper" &&
            "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
          className
        )}
        position={position}
        align={align}
        {...props}
      >
        <SelectScrollUpButton />
        <SelectPrimitive.Viewport
          className={cn(
            "p-1",
            position === "popper" &&
              "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)] scroll-my-1"
          )}
        >
          {children}
        </SelectPrimitive.Viewport>
        <SelectScrollDownButton />
      </SelectPrimitive.Content>
    </SelectPrimitive.Portal>
  )
}

function SelectLabel({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Label>) {
  return (
    <SelectPrimitive.Label
      data-slot="select-label"
      className={cn("text-muted-foreground px-2 py-1.5 text-xs", className)}
      {...props}
    />
  )
}

function SelectItem({
  className,
  children,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Item>) {
  return (
    <SelectPrimitive.Item
      data-slot="select-item"
      className={cn(
        "focus:bg-accent focus:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground relative flex w-full cursor-default items-center gap-2 rounded-sm py-1.5 pr-8 pl-2 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4 *:[span]:last:flex *:[span]:last:items-center *:[span]:last:gap-2",
        className
      )}
      {...props}
    >
      <span className="absolute right-2 flex size-3.5 items-center justify-center">
        <SelectPrimitive.ItemIndicator>
          <CheckIcon className="size-4" />
        </SelectPrimitive.ItemIndicator>
      </span>
      <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
    </SelectPrimitive.Item>
  )
}

function SelectSeparator({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Separator>) {
  return (
    <SelectPrimitive.Separator
      data-slot="select-separator"
      className={cn("bg-border pointer-events-none -mx-1 my-1 h-px", className)}
      {...props}
    />
  )
}

function SelectScrollUpButton({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.ScrollUpButton>) {
  return (
    <SelectPrimitive.ScrollUpButton
      data-slot="select-scroll-up-button"
      className={cn(
        "flex cursor-default items-center justify-center py-1",
        className
      )}
      {...props}
    >
      <ChevronUpIcon className="size-4" />
    </SelectPrimitive.ScrollUpButton>
  )
}

function SelectScrollDownButton({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.ScrollDownButton>) {
  return (
    <SelectPrimitive.ScrollDownButton
      data-slot="select-scroll-down-button"
      className={cn(
        "flex cursor-default items-center justify-center py-1",
        className
      )}
      {...props}
    >
      <ChevronDownIcon className="size-4" />
    </SelectPrimitive.ScrollDownButton>
  )
}

export {
  Select,
  SelectContent,
  SelectGroup,
  SelectItem,
  SelectLabel,
  SelectScrollDownButton,
  SelectScrollUpButton,
  SelectSeparator,
  SelectTrigger,
  SelectValue,
}
</file>

<file path="frontend/src/components/ui/slider.tsx">
import * as React from "react"
import * as SliderPrimitive from "@radix-ui/react-slider"

import { cn } from "@/lib/utils"

function Slider({
  className,
  defaultValue,
  value,
  min = 0,
  max = 100,
  ...props
}: React.ComponentProps<typeof SliderPrimitive.Root>) {
  const _values = React.useMemo(
    () =>
      Array.isArray(value)
        ? value
        : Array.isArray(defaultValue)
          ? defaultValue
          : [min, max],
    [value, defaultValue, min, max]
  )

  return (
    <SliderPrimitive.Root
      data-slot="slider"
      defaultValue={defaultValue}
      value={value}
      min={min}
      max={max}
      className={cn(
        "relative flex w-full touch-none items-center select-none data-[disabled]:opacity-50 data-[orientation=vertical]:h-full data-[orientation=vertical]:min-h-44 data-[orientation=vertical]:w-auto data-[orientation=vertical]:flex-col",
        className
      )}
      {...props}
    >
      <SliderPrimitive.Track
        data-slot="slider-track"
        className={cn(
          "bg-muted relative grow overflow-hidden rounded-full data-[orientation=horizontal]:h-1.5 data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-1.5"
        )}
      >
        <SliderPrimitive.Range
          data-slot="slider-range"
          className={cn(
            "bg-primary absolute data-[orientation=horizontal]:h-full data-[orientation=vertical]:w-full"
          )}
        />
      </SliderPrimitive.Track>
      {Array.from({ length: _values.length }, (_, index) => (
        <SliderPrimitive.Thumb
          data-slot="slider-thumb"
          key={index}
          className="border-primary ring-ring/50 block size-4 shrink-0 rounded-full border bg-white shadow-sm transition-[color,box-shadow] hover:ring-4 focus-visible:ring-4 focus-visible:outline-hidden disabled:pointer-events-none disabled:opacity-50"
        />
      ))}
    </SliderPrimitive.Root>
  )
}

export { Slider }
</file>

<file path="frontend/src/components/ui/textarea.tsx">
import * as React from "react"

import { cn } from "@/lib/utils"

function Textarea({ className, ...props }: React.ComponentProps<"textarea">) {
  return (
    <textarea
      data-slot="textarea"
      className={cn(
        "border-input placeholder:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 flex field-sizing-content min-h-16 w-full rounded-md border bg-transparent px-3 py-2 text-base shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        className
      )}
      {...props}
    />
  )
}

export { Textarea }
</file>

<file path="frontend/src/lib/utils.ts">
import { clsx, type ClassValue } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
</file>

<file path="frontend/src/App.css">
#root {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em #646cffaa);
}
.logo.react:hover {
  filter: drop-shadow(0 0 2em #61dafbaa);
}

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: #888;
}
</file>

<file path="frontend/src/index.css">
@import "tailwindcss";
@import "tw-animate-css";

@custom-variant dark (&:is(.dark *));

@theme inline {
  --radius-sm: calc(var(--radius) - 4px);
  --radius-md: calc(var(--radius) - 2px);
  --radius-lg: var(--radius);
  --radius-xl: calc(var(--radius) + 4px);
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --color-card: var(--card);
  --color-card-foreground: var(--card-foreground);
  --color-popover: var(--popover);
  --color-popover-foreground: var(--popover-foreground);
  --color-primary: var(--primary);
  --color-primary-foreground: var(--primary-foreground);
  --color-secondary: var(--secondary);
  --color-secondary-foreground: var(--secondary-foreground);
  --color-muted: var(--muted);
  --color-muted-foreground: var(--muted-foreground);
  --color-accent: var(--accent);
  --color-accent-foreground: var(--accent-foreground);
  --color-destructive: var(--destructive);
  --color-border: var(--border);
  --color-input: var(--input);
  --color-ring: var(--ring);
  --color-chart-1: var(--chart-1);
  --color-chart-2: var(--chart-2);
  --color-chart-3: var(--chart-3);
  --color-chart-4: var(--chart-4);
  --color-chart-5: var(--chart-5);
  --color-sidebar: var(--sidebar);
  --color-sidebar-foreground: var(--sidebar-foreground);
  --color-sidebar-primary: var(--sidebar-primary);
  --color-sidebar-primary-foreground: var(--sidebar-primary-foreground);
  --color-sidebar-accent: var(--sidebar-accent);
  --color-sidebar-accent-foreground: var(--sidebar-accent-foreground);
  --color-sidebar-border: var(--sidebar-border);
  --color-sidebar-ring: var(--sidebar-ring);
}

:root {
  --radius: 0.625rem;
  --background: oklch(1 0 0);
  --foreground: oklch(0.145 0 0);
  --card: oklch(1 0 0);
  --card-foreground: oklch(0.145 0 0);
  --popover: oklch(1 0 0);
  --popover-foreground: oklch(0.145 0 0);
  --primary: oklch(0.205 0 0);
  --primary-foreground: oklch(0.985 0 0);
  --secondary: oklch(0.97 0 0);
  --secondary-foreground: oklch(0.205 0 0);
  --muted: oklch(0.97 0 0);
  --muted-foreground: oklch(0.556 0 0);
  --accent: oklch(0.97 0 0);
  --accent-foreground: oklch(0.205 0 0);
  --destructive: oklch(0.577 0.245 27.325);
  --border: oklch(0.922 0 0);
  --input: oklch(0.922 0 0);
  --ring: oklch(0.708 0 0);
  --chart-1: oklch(0.646 0.222 41.116);
  --chart-2: oklch(0.6 0.118 184.704);
  --chart-3: oklch(0.398 0.07 227.392);
  --chart-4: oklch(0.828 0.189 84.429);
  --chart-5: oklch(0.769 0.188 70.08);
  --sidebar: oklch(0.985 0 0);
  --sidebar-foreground: oklch(0.145 0 0);
  --sidebar-primary: oklch(0.205 0 0);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.97 0 0);
  --sidebar-accent-foreground: oklch(0.205 0 0);
  --sidebar-border: oklch(0.922 0 0);
  --sidebar-ring: oklch(0.708 0 0);
}

.dark {
  --background: oklch(0.145 0 0);
  --foreground: oklch(0.985 0 0);
  --card: oklch(0.205 0 0);
  --card-foreground: oklch(0.985 0 0);
  --popover: oklch(0.205 0 0);
  --popover-foreground: oklch(0.985 0 0);
  --primary: oklch(0.922 0 0);
  --primary-foreground: oklch(0.205 0 0);
  --secondary: oklch(0.269 0 0);
  --secondary-foreground: oklch(0.985 0 0);
  --muted: oklch(0.269 0 0);
  --muted-foreground: oklch(0.708 0 0);
  --accent: oklch(0.269 0 0);
  --accent-foreground: oklch(0.985 0 0);
  --destructive: oklch(0.704 0.191 22.216);
  --border: oklch(1 0 0 / 10%);
  --input: oklch(1 0 0 / 15%);
  --ring: oklch(0.556 0 0);
  --chart-1: oklch(0.488 0.243 264.376);
  --chart-2: oklch(0.696 0.17 162.48);
  --chart-3: oklch(0.769 0.188 70.08);
  --chart-4: oklch(0.627 0.265 303.9);
  --chart-5: oklch(0.645 0.246 16.439);
  --sidebar: oklch(0.205 0 0);
  --sidebar-foreground: oklch(0.985 0 0);
  --sidebar-primary: oklch(0.488 0.243 264.376);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.269 0 0);
  --sidebar-accent-foreground: oklch(0.985 0 0);
  --sidebar-border: oklch(1 0 0 / 10%);
  --sidebar-ring: oklch(0.556 0 0);
}

@layer base {
  * {
    @apply border-border outline-ring/50;
  }
  body {
    @apply bg-background text-foreground;
  }
}
</file>

<file path="frontend/src/main.tsx">
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.tsx'

createRoot(document.getElementById('root')!).render(
  <StrictMode>
    <App />
  </StrictMode>,
)
</file>

<file path="frontend/components.json">
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "new-york",
  "rsc": false,
  "tsx": true,
  "tailwind": {
    "config": "",
    "css": "src/index.css",
    "baseColor": "neutral",
    "cssVariables": true,
    "prefix": ""
  },
  "iconLibrary": "lucide",
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils",
    "ui": "@/components/ui",
    "lib": "@/lib",
    "hooks": "@/hooks"
  },
  "registries": {}
}
</file>

<file path="frontend/Dockerfile.frontend">
# Stage 1: Build the React application
FROM node:20-alpine AS build

WORKDIR /app

# Copy package files and install dependencies
COPY package.json package-lock.json ./
RUN npm install

# Copy the rest of the application source code
COPY . .

# Build the application
RUN npm run build

# Stage 2: Serve the application with Nginx
FROM nginx:1.25-alpine AS final

# Copy the built assets from the build stage
COPY --from=build /app/dist /usr/share/nginx/html

# Copy the custom Nginx configuration
# We will create this file next
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port 80
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]
</file>

<file path="frontend/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>frontend</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="frontend/package.json">
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@radix-ui/react-label": "^2.1.7",
    "@radix-ui/react-select": "^2.2.6",
    "@radix-ui/react-slider": "^1.3.6",
    "@radix-ui/react-slot": "^1.2.3",
    "@tailwindcss/vite": "^4.1.14",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "lucide-react": "^0.545.0",
    "react": "^19.1.1",
    "react-dom": "^19.1.1",
    "tailwind-merge": "^3.3.1",
    "tailwindcss": "^4.1.14"
  },
  "devDependencies": {
    "@eslint/js": "^9.36.0",
    "@types/node": "^24.7.2",
    "@types/react": "^19.1.16",
    "@types/react-dom": "^19.1.9",
    "@vitejs/plugin-react": "^5.0.4",
    "eslint": "^9.36.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.22",
    "globals": "^16.4.0",
    "tw-animate-css": "^1.4.0",
    "typescript": "~5.9.3",
    "typescript-eslint": "^8.45.0",
    "vite": "^7.1.7"
  }
}
</file>

<file path="frontend/README.md">
# React + TypeScript + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

## React Compiler

The React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).

## Expanding the ESLint configuration

If you are developing a production application, we recommend updating the configuration to enable type-aware lint rules:

```js
export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      // Other configs...

      // Remove tseslint.configs.recommended and replace with this
      tseslint.configs.recommendedTypeChecked,
      // Alternatively, use this for stricter rules
      tseslint.configs.strictTypeChecked,
      // Optionally, add this for stylistic rules
      tseslint.configs.stylisticTypeChecked,

      // Other configs...
    ],
    languageOptions: {
      parserOptions: {
        project: ['./tsconfig.node.json', './tsconfig.app.json'],
        tsconfigRootDir: import.meta.dirname,
      },
      // other options...
    },
  },
])
```

You can also install [eslint-plugin-react-x](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-x) and [eslint-plugin-react-dom](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-dom) for React-specific lint rules:

```js
// eslint.config.js
import reactX from 'eslint-plugin-react-x'
import reactDom from 'eslint-plugin-react-dom'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      // Other configs...
      // Enable lint rules for React
      reactX.configs['recommended-typescript'],
      // Enable lint rules for React DOM
      reactDom.configs.recommended,
    ],
    languageOptions: {
      parserOptions: {
        project: ['./tsconfig.node.json', './tsconfig.app.json'],
        tsconfigRootDir: import.meta.dirname,
      },
      // other options...
    },
  },
])
```
</file>

<file path="frontend/tsconfig.app.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "types": ["vite/client"],
    "skipLibCheck": true,
    "baseUrl": ".",
    "paths": {
      "@/*": [
        "./src/*"
      ]
    },

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}
</file>

<file path="frontend/tsconfig.json">
{
  "files": [],
  "references": [
    {
      "path": "./tsconfig.app.json"
    },
    {
      "path": "./tsconfig.node.json"
    }
  ],
  "compilerOptions": {
    "baseUrl": ".",
    "paths": {
      "@/*": ["./src/*"]
    }
  }
}
</file>

<file path="frontend/tsconfig.node.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "types": ["node"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="tests/test_api_generate.py">
from fastapi.testclient import TestClient
from unittest.mock import MagicMock

from app.main import app
from app.core.config import settings

client = TestClient(app)

# Get a valid key from the settings
VALID_API_KEY = list(settings.API_KEYS)[0]

def test_enqueue_generate_success(monkeypatch):
    """Test successful enqueue of a generation job."""
    # Mock the celery task's delay method
    mock_delay = MagicMock()
    mock_delay.return_value = MagicMock(id="test-job-123")
    monkeypatch.setattr("app.worker.generate_text_task.delay", mock_delay)

    payload = {
        "model": "qwen3:0.6b",
        "prompt": "Why is the sky blue?",
        "max_tokens": 100,
    }

    response = client.post(
        "/generate",
        headers={"x-api-key": VALID_API_KEY},
        json=payload,
    )

    assert response.status_code == 200
    assert response.json() == {"job_id": "test-job-123", "status": "queued"}

    # Verify that the task was called correctly
    mock_delay.assert_called_once_with(
        payload["model"],
        payload["prompt"],
        payload["max_tokens"],
        0.7,  # temperature defaults to 0.7 in the pydantic model
    )

def test_enqueue_generate_no_api_key():
    """Test request without an API key."""
    response = client.post("/generate", json={})
    assert response.status_code == 401
    assert response.json() == {"detail": "Missing or invalid API key"}

def test_enqueue_generate_invalid_api_key():
    """Test request with an invalid API key."""
    response = client.post(
        "/generate",
        headers={"x-api-key": "invalid-key"},
        json={},
    )
    assert response.status_code == 401
    assert response.json() == {"detail": "Missing or invalid API key"}
</file>

<file path="tests/test_api_models.py">
import pytest

from fastapi.testclient import TestClient

import httpx



from app.main import app





def test_list_models_success(monkeypatch):

    """Test successful listing of models by monkeypatching the client method."""



    async def mock_list_models(*args, **kwargs):

        return ["qwen3:0.6b", "mistral:latest"]



    monkeypatch.setattr("app.services.ollama_client.OllamaClient.list_models", mock_list_models)



    with TestClient(app) as client:

        response = client.get("/models")



    assert response.status_code == 200

    assert response.json() == {"models": ["qwen3:0.6b", "mistral:latest"]}





def test_list_models_ollama_error(monkeypatch):

    """Test an error case by having the monkeypatched method raise an exception."""



    async def mock_list_models_error(*args, **kwargs):

        raise httpx.RequestError("Mocked network error")



    monkeypatch.setattr("app.services.ollama_client.OllamaClient.list_models", mock_list_models_error)



    with TestClient(app) as client:

        response = client.get("/models")



    assert response.status_code == 500

    assert "Mocked network error" in response.json()["detail"]
</file>

<file path="tests/test_main.py">
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_healthz():
    """Test the health check endpoint."""
    response = client.get("/healthz")
    assert response.status_code == 200
    assert response.json() == {"ok": True}
</file>

<file path="Dockerfile">
# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Set environment variables
# Prevents Python from writing pyc files to disc
ENV PYTHONDONTWRITEBYTECODE 1
# Ensures Python output is sent straight to the terminal without buffering
ENV PYTHONUNBUFFERED 1
# Set the python path to include the project root
ENV PYTHONPATH=.

# Set the working directory in the container
WORKDIR /app

# Install dependencies
# Copy the requirements file first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code into the container
COPY ./app /app/app

# The command to run the application will be specified in docker-compose.yml
</file>

<file path="app/api/routes_generate.py">
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from app.core.auth import require_api_key
from app.worker import generate_text_task

router = APIRouter()

class GenerateRequest(BaseModel):
    model: str = "qwen3:0.6b"
    prompt: str
    max_tokens: int | None = None
    temperature: float | None = 0.7
    top_p: float | None = 0.9
    stream: bool = False

@router.post("", dependencies=[Depends(require_api_key)])
async def enqueue_generate(req: GenerateRequest):
    """Enqueue text generation job"""
    task = generate_text_task.delay(req.model, req.prompt, req.max_tokens, req.temperature)
    return {"job_id": task.id, "status": "queued"}

# @router.post("/", dependencies=[Depends(require_api_key)])
# async def generate(req: GenerateRequest):
#     options = {}
#     if req.max_tokens is not None: options["num_predict"] = req.max_tokens
#     if req.temperature is not None: options["temperature"] = req.temperature
#     if req.top_p is not None: options["top_p"] = req.top_p

#     if not req.stream:
#         try:
#             data = await ollama.generate_once(req.model, req.prompt, options)
#             return JSONResponse(data)
#         except Exception as e:
#             raise HTTPException(status_code=500, detail=str(e))

#     async def gen():
#         try:
#             async for line in ollama.stream_generate(req.model, req.prompt, options):
#                 yield line + "\n"
#         except Exception as e:
#             yield json.dumps({"error": str(e)}) + "\n"

#     return StreamingResponse(gen(), media_type="application/x-ndjson")
</file>

<file path="app/api/routes_models.py">
# app/api/routes_models.py
from fastapi import APIRouter, HTTPException, Request

router = APIRouter()

@router.get("")  
async def list_models(request: Request):
    try:
        client = request.app.state.ollama  # ‚úÖ use app.state
        models = await client.list_models()
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="app/api/routes_status.py">
# app/api/routes_status.py
from fastapi import APIRouter
from celery.result import AsyncResult
from app.worker import celery_app

router = APIRouter()

@router.get("/{job_id}")
async def get_status(job_id: str):
    """Return Celery job status and result."""
    result = AsyncResult(job_id, app=celery_app)

    if result.state == "PENDING":
        return {"status": "queued"}
    elif result.state == "STARTED":
        return {"status": "processing"}
    elif result.state == "SUCCESS":
        return {
            "status": "completed",
            "result": result.result,  # {'model': '...', 'text': '...'}
        }
    elif result.state == "FAILURE":
        return {"status": "failed", "error": str(result.result)}
    else:
        return {"status": result.state.lower()}
</file>

<file path="app/main.py">
# app/main.py

from fastapi import FastAPI, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from starlette.middleware.base import BaseHTTPMiddleware
import asyncio, time
from app.api import routes_generate, routes_models, routes_status
from app.core.config import settings
from app.core.logger import logger
from app.core.metrics import render_metrics, LATENCY, REQUESTS, ERRORS
from app.services.ollama_client import OllamaClient  # ‚úÖ use the class

app = FastAPI(title="Prodify Gateway", version="3.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

sem = asyncio.Semaphore(settings.CONCURRENCY_LIMIT)

class MetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        if request.url.path in ("/healthz", "/readyz", "/metrics", "/static/index.html"):
            return await call_next(request)

        model = "unknown"
        if request.headers.get("content-type") == "application/json":
            try:
                body_bytes = await request.body()
                body = body_bytes.decode("utf-8")
                # avoid consuming body: rebuild scope receive
                async def receive():
                    return {"type": "http.request", "body": body_bytes}
                request = Request(request.scope, receive)
                import json
                model = json.loads(body).get("model", "unknown")
            except:
                pass

        route = request.url.path
        start = time.perf_counter()
        resp = None
        try:
            async with sem:
                resp = await call_next(request)
            return resp
        finally:
            if resp is not None:
                LATENCY.labels(route=route, model=model).observe(time.perf_counter() - start)
                REQUESTS.labels(route=route, model=model).inc()

app.add_middleware(MetricsMiddleware)

# Routers
app.include_router(routes_models.router, prefix="/models", tags=["Models"])
app.include_router(routes_generate.router, prefix="/generate", tags=["Generate"])
app.include_router(routes_status.router, prefix="/status", tags=["Status"])

# Static
app.mount("/static", StaticFiles(directory="app/static"), name="static")

@app.on_event("startup")
async def on_startup():
    logger.info("üöÄ Gateway starting...")
    # ‚úÖ create a client for the API process and keep it in app.state
    app.state.ollama = OllamaClient()
    try:
        models = await app.state.ollama.list_models()
        logger.info(f"‚úÖ Ollama reachable. Models available: {models}")
    except Exception as e:
        logger.error(f"‚ùå Ollama not reachable on startup: {e}")

@app.on_event("shutdown")
async def on_shutdown():
    logger.info("üåô Gateway shutting down.")
    if hasattr(app.state, "ollama"):
        await app.state.ollama.close()

@app.get("/healthz")
async def healthz():
    return {"ok": True}

@app.get("/readyz")
async def readyz():
    try:
        models = await app.state.ollama.list_models()  # ‚úÖ use app.state
        return {"ready": True, "models": len(models)}
    except Exception as e:
        return {"ready": False, "error": str(e)}

@app.get("/metrics")
async def metrics():
    body, ctype = render_metrics()
    return Response(content=body, media_type=ctype)
</file>

<file path="frontend/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local
.env*

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
</file>

<file path="frontend/nginx.conf">
server {
  listen 80;

  # Root directory for static files
  root /usr/share/nginx/html;
  index index.html;

  # Handle client-side routing for React
  location / {
    try_files $uri $uri/ /index.html;
  }

  # Proxy API requests to the backend service
  location ~ ^/(models|generate|status) {
    proxy_pass http://backend:8000;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header x-api-key $http_x_api_key;
  }

  # Optional: Add headers to prevent caching in development
  location ~* \.(?:html|js|css)$ {
    add_header Cache-Control "no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0";
    expires off;
    etag off;
  }
}
</file>

<file path="frontend/vite.config.ts">
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import path from "path"
import tailwindcss from "@tailwindcss/vite"

// https://vite.dev/config/
export default defineConfig({
  plugins: [react(), tailwindcss()],
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
    },
  },
  server: {
    proxy: {
      "/generate": "http://127.0.0.1:8000",
      "/models": "http://127.0.0.1:8000",
      "/status": "http://127.0.0.1:8000",
    },
  },
  
})
</file>

<file path=".env.example">
# API keys for the backend, comma-separated
API_KEYS=your-development-api-key
</file>

<file path=".gitignore">
.venv
__pycache__
.env
dump.rdb
</file>

<file path="README.md">
# üöÄ Prodify AI Inference Gateway

> Scalable async inference system using **FastAPI**, **Celery**, **Redis**, and **Ollama**, with a modern **React (Vite + Shadcn)** frontend.

---

## üß† System Overview

```
Frontend (React)
   ‚îÇ  prompts / polls
   ‚ñº
FastAPI Gateway (API + Auth)
   ‚îÇ  queues task
   ‚ñº
Redis (Broker + Backend)
   ‚îÇ  distributes
   ‚ñº
Celery Worker ‚Üí Ollama (LLM)
   ‚îÇ  results
   ‚ñº
Frontend UI updates
```

---

## ‚öôÔ∏è Tech Stack

| Layer            | Tech           | Role                         |
| ---------------- | -------------- | ---------------------------- |
| **Frontend**     | React + Shadcn | Async prompt + polling UI    |
| **Gateway**      | FastAPI        | Auth, routing, metrics       |
| **Queue**        | Celery         | Async job management         |
| **Broker/Store** | Redis          | Queue + result storage       |
| **Inference**    | Ollama         | Local models (Qwen, Mistral) |

---

## üìÅ Folder Structure

```
app/
 ‚î£ api/
 ‚îÉ ‚î£ routes_generate.py   # POST /generate (async job)
 ‚îÉ ‚î£ routes_status.py     # GET /status/{job_id}
 ‚îÉ ‚îó routes_models.py     # GET /models
 ‚î£ services/
 ‚îÉ ‚îó ollama_client.py     # Ollama API wrapper
 ‚î£ worker.py              # Celery task runner
 ‚îó main.py                # FastAPI entrypoint
frontend/
 ‚îó src/App.tsx            # Async UI (model + prompt)
```

---

## ‚ö° Workflow

1. Frontend calls `/generate` ‚Üí returns job_id
2. FastAPI enqueues Celery task ‚Üí Redis
3. Worker pulls task ‚Üí calls Ollama ‚Üí stores result
4. Frontend polls `/status/{job_id}` ‚Üí displays output

---

## üß© Example

```bash
curl -X POST http://127.0.0.1:8000/generate \
 -H "x-api-key: dev-key-123" \
 -d '{"model":"qwen3:0.6b","prompt":"Write a haiku about Bangalore."}'
```

```json
{ "job_id": "123abc", "status": "queued" }
```

Later:

```json
{ "status": "completed", "result": { "text": "Bangalore hums bright..." } }
```

---

## üöÄ Highlights

- Async inference via **Celery + Redis**
- Local models served with **Ollama**
- Secure API-key auth
- Modern frontend with **Vite + Shadcn**
- Scalable, fault-tolerant design
</file>

<file path="app/services/ollama_client.py">
import httpx
import json
import asyncio
import logging

from app.core.config import settings

logger = logging.getLogger(__name__)

class OllamaClient:
    def __init__(self, base_url: str | None = None):
        """Initialize a self-contained Ollama HTTP client."""
        self.base_url = base_url or settings.OLLAMA_HOST
        # immediately create an AsyncClient ‚Äî no .connect() required
        self.client = httpx.AsyncClient(timeout=120)

    async def list_models(self):
        """Fetch available Ollama models."""
        try:
            r = await self.client.get(f"{self.base_url}/api/tags")
            r.raise_for_status()
            data = r.json()
            return [m["name"] for m in data.get("models", [])]
        except Exception as e:
            logger.error(f"Error fetching Ollama models: {e}")
            return []

    async def generate_once(self, model: str, prompt: str, options: dict = None):
        """Stream inference output and combine into final text."""
        options = options or {}
        payload = {
            "model": model,
            "prompt": prompt,
            "options": options,
            "stream": True
        }

        full_text = ""
        try:
            async with self.client.stream("POST", f"{self.base_url}/api/generate", json=payload) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    try:
                        data = json.loads(line)
                        if "response" in data and data["response"]:
                            full_text += data["response"]
                    except Exception:
                        continue
        except Exception as e:
            logger.error(f"Error during Ollama generation: {e}")
            raise
        return {"model": model, "text": full_text.strip()}

    async def close(self):
        """Cleanly close the HTTP connection."""
        await self.client.aclose()
</file>

<file path="frontend/eslint.config.js">
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import tseslint from 'typescript-eslint'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      js.configs.recommended,
      tseslint.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    rules: {
        'react-refresh/only-export-components': [
            'warn',
            { allowConstantExport: true },
        ],
    },
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
  },
])
</file>

<file path="requirements.txt">
amqp==5.3.1
annotated-types==0.7.0
anyio==4.11.0
billiard==4.2.2
celery==5.5.3
certifi==2025.10.5
charset-normalizer==3.4.4
click==8.3.0
click-didyoumean==0.3.1
click-plugins==1.1.1.2
click-repl==0.3.0
fastapi==0.119.0
filelock==3.20.0
flower==2.0.1
fsspec==2025.9.0
h11==0.16.0
hf-xet==1.1.10
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.35.3
humanize==4.14.0
idna==3.11
iniconfig==2.1.0
jinja2==3.1.6
kombu==5.5.4
markupsafe==3.0.3
mpmath==1.3.0
networkx==3.5
numpy==2.3.3
packaging==25.0
pluggy==1.6.0
prometheus-client==0.23.1
prompt-toolkit==3.0.52
pydantic==2.12.2
pydantic-core==2.41.4
pygments==2.19.2
pytest==8.4.2
pytest-asyncio==1.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.1.1
pytz==2025.2
pyyaml==6.0.3
redis==5.2.1
regex==2025.9.18
requests==2.32.5
respx==0.22.0
safetensors==0.6.2
setuptools==80.9.0
six==1.17.0
sniffio==1.3.1
starlette==0.48.0
sympy==1.14.0
tokenizers==0.22.1
torch==2.8.0
tornado==6.5.2
tqdm==4.67.1
transformers==4.57.1
typing-extensions==4.15.0
typing-inspection==0.4.2
tzdata==2025.2
urllib3==2.5.0
uvicorn==0.37.0
vine==5.1.0
wcwidth==0.2.14
</file>

<file path="app/core/celery_app.py">
from celery import Celery
import os

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

celery_app = Celery(
    "prodify_ai",
    broker=REDIS_URL,
    backend=REDIS_URL,
)

celery_app.conf.update(
    task_track_started=True,
    task_serializer="json",
    result_serializer="json",
    accept_content=["json"],
    timezone="UTC",
    enable_utc=True,
    broker_connection_retry_on_startup=True,
    task_acks_late=True,
    broker_transport_options={"visibility_timeout": 7200},
    task_default_retry_delay=10,
    task_annotations={
        '*': {'max_retries': 3, 'time_limit': 300}
    },
)
</file>

<file path="app/worker.py">
from app.core.celery_app import celery_app
from app.services.ollama_client import OllamaClient
import asyncio
from app.core.logger import logger
import time

@celery_app.task(bind=True, name="generate_text_task")
def generate_text_task(self, model: str, prompt: str, max_tokens: int = 200, temperature: float = 0.7):
    """Celery worker task: independent Ollama connection."""
    async def _run():
        ollama = OllamaClient()
        # self.update_state(state="STARTED", meta={"progress": 0})
        logger.info(f"[{self.request.id}] Starting text generation at {time.time()}")
        result = await ollama.generate_once(
            model=model,
            prompt=prompt,
            options={"num_predict": max_tokens, "temperature": temperature},
        )
        logger.info(f"[{self.request.id}] Finished text generation at {time.time()}")
        await ollama.close()
        return result

    return asyncio.run(_run())
</file>

<file path="frontend/src/App.tsx">
import { useEffect, useState } from "react"
import { Button } from "@/components/ui/button"
import { Textarea } from "@/components/ui/textarea"
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select"
import { Slider } from "@/components/ui/slider"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"
import { Label } from "@/components/ui/label"

function App() {
  const [models, setModels] = useState<string[]>([])
  const [model, setModel] = useState("")
  const [prompt, setPrompt] = useState("Once upon a time in Bangalore,")
  const [maxTokens, setMaxTokens] = useState(200)
  const [loading, setLoading] = useState(false)
  const [output, setOutput] = useState("")
  const [jobId, setJobId] = useState<string | null>(null)
  const [status, setStatus] = useState("idle")

  useEffect(() => {
    fetch("/models")
      .then((res) => res.json())
      .then((data) => {
        setModels(data.models || [])
        if (data.models?.length > 0) setModel(data.models[0])
      })
      .catch((err) => console.error("Failed to load models:", err))
  }, [])

  async function handleGenerate() {
    if (!model || !prompt.trim()) return
    setLoading(true)
    setOutput("")
    setJobId(null)
    setStatus("queued")

    try {
      const res = await fetch("/generate", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "x-api-key": import.meta.env.VITE_API_KEY,
        },
        body: JSON.stringify({ model, prompt, max_tokens: maxTokens }),
      })

      const data = await res.json()
      if (data.job_id) {
        setJobId(data.job_id)
        pollStatus(data.job_id)
      } else {
        setOutput("Unexpected response: " + JSON.stringify(data))
        setLoading(false)
      }
    } catch (err) {
      console.error(err)
      setOutput("Error generating text.")
      setLoading(false)
    }
  }

  async function pollStatus(jobId: string) {
    const interval = setInterval(async () => {
      const res = await fetch(`/status/${jobId}`)
      const data = await res.json()

      if (data.status === "completed") {
        clearInterval(interval)
        setOutput(data.result.text || "No text returned.")
        setStatus("completed")
        setLoading(false)
      } else if (data.status === "failed") {
        clearInterval(interval)
        setOutput(`‚ùå Failed: ${data.error}`)
        setStatus("failed")
        setLoading(false)
      } else {
        setStatus(data.status)
      }
    }, 2000)
  }

  return (
    <div className="min-h-screen bg-gray-50 flex flex-col items-center py-10 px-4">
      <div className="w-full max-w-2xl space-y-6">
        <h1 className="text-3xl font-bold text-center">üöÄ Prodify Text Inference Queue</h1>
        <p className="text-center text-gray-600">
          Generate text asynchronously via FastAPI + Celery + Ollama.
        </p>

        {/* Model Selector */}
        <div className="space-y-2">
          <Label>Select Model</Label>
          <Select value={model} onValueChange={setModel}>
            <SelectTrigger>
              <SelectValue placeholder="Choose a model..." />
            </SelectTrigger>
            <SelectContent>
              {models.map((m) => (
                <SelectItem key={m} value={m}>
                  {m}
                </SelectItem>
              ))}
            </SelectContent>
          </Select>
        </div>

        {/* Prompt Input */}
        <div className="space-y-2">
          <Label>Prompt</Label>
          <Textarea
            value={prompt}
            onChange={(e) => setPrompt(e.target.value)}
            placeholder="Enter your text prompt..."
            rows={5}
          />
        </div>

        {/* Token Slider */}
        <div className="space-y-2">
          <Label>Max Tokens: {maxTokens}</Label>
          <Slider
            value={[maxTokens]}
            onValueChange={(val) => setMaxTokens(val[0])}
            min={10}
            max={500}
            step={10}
          />
        </div>

        {/* Generate Button */}
        <div className="flex justify-center">
          <Button onClick={handleGenerate} disabled={loading || !model}>
            {loading ? `Generating (${status})...` : "Generate"}
          </Button>
        </div>

        {/* Output Card */}
        <Card className="mt-6">
          <CardHeader>
            <CardTitle>Output</CardTitle>
          </CardHeader>
          <CardContent>
            {jobId && (
              <div className="text-xs text-gray-500 mb-2">
                Job ID: <code>{jobId}</code> ({status})
              </div>
            )}
            <pre className="whitespace-pre-wrap text-sm">
              {output || "No output yet."}
            </pre>
          </CardContent>
        </Card>
      </div>
    </div>
  )
}

export default App
</file>

<file path="docker-compose.yml">
services:
  # Redis service for Celery message broker and result backend
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 5s
      timeout: 10s
      retries: 5
    environment:
      - OLLAMA_LOAD_TIMEOUT=600

  # Backend FastAPI server
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    cache:
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-backend:latest
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-backend:main,mode=max
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - API_KEYS=${API_KEYS}
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy

  # Celery worker for background tasks
  worker:
    build:
      context: .
      dockerfile: Dockerfile
    cache:
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-frontend:latest
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-frontend:main,mode=max
    command: celery -A app.worker.celery_app worker --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy

  # Frontend Nginx server
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.frontend
    ports:
      - "8080:80"
    depends_on:
      backend:
        condition: service_started

volumes:
  ollama_data:
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  test-backend:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run tests
        run: PYTHONPATH=. pytest

  test-frontend:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: ./frontend
        run: npm install

      - name: Lint frontend
        working-directory: ./frontend
        run: npm run lint

      - name: Build frontend
        working-directory: ./frontend
        run: npm run build

  integration-test:
    runs-on: ubuntu-latest
    needs: [test-backend, test-frontend]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ‚¨áÔ∏è STEP 1: ADDED - Log in to GHCR to store/retrieve cache layers
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # ‚¨áÔ∏è STEP 2: ADDED - Set up the advanced builder required for caching
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3


      - name: Build and run Docker Compose
        env:
          API_KEYS: dev-key-123
        run: docker compose up -d --build

      - name: Install jq
        run: sudo apt-get install -y jq

      - name: Wait for services to be healthy
        run: |
          echo "Waiting for backend..."
          while [[ "$(curl -s -o /dev/null -w '%{http_code}' http://localhost:8000/healthz)" != "200" ]]; do
            sleep 2
          done
          echo "Waiting for frontend..."
          while [[ "$(curl -s -o /dev/null -w '%{http_code}' http://localhost:8080/)" != "200" ]]; do
            sleep 2
          done
          echo "Services are up!"

      - name: Run End-to-End Test
        env:
          API_KEYS: dev-key-123
        run: |
          # (Your test script remains the same)
          echo "Starting end-to-end test for /generate endpoint..."

          HTTP_RESPONSE=$(curl -s -w "\n%{http_code}" -X POST -H "Content-Type: application/json" -H "x-api-key: $API_KEYS" -d '{"model":"qwen3:0.6b","prompt":"a haiku about CI/CD"}' http://localhost:8080/generate)
          HTTP_STATUS=$(echo "$HTTP_RESPONSE" | tail -n1)
          HTTP_BODY=$(echo "$HTTP_RESPONSE" | sed '$d')

          echo "Response Body: $HTTP_BODY"
          echo "Response Status: $HTTP_STATUS"

          if [ "$HTTP_STATUS" -ne 200 ]; then
            echo "/generate endpoint failed with status $HTTP_STATUS"
            exit 1
          fi

          JOB_ID=$(echo $HTTP_BODY | jq -r '.job_id')

          if [ -z "$JOB_ID" ] || [ "$JOB_ID" == "null" ]; then
            echo "Failed to get a valid job_id from response!"
            exit 1
          fi
          echo "Job submitted with ID: $JOB_ID"

          echo "Polling for job completion..."
          for i in {1..60}; do
            STATUS_RESPONSE=$(curl -s http://localhost:8080/status/$JOB_ID)
            JOB_STATUS=$(echo $STATUS_RESPONSE | jq -r '.status')
            echo "Current job status: $JOB_STATUS"

            if [ "$JOB_STATUS" == "completed" ]; then
              echo "Job completed successfully!"
              RESULT_TEXT=$(echo $STATUS_RESPONSE | jq -r '.result.text')
              echo "Result: $RESULT_TEXT"
              exit 0
            elif [ "$JOB_STATUS" == "failed" ]; then
              echo "Job failed!"
              echo "Response: $STATUS_RESPONSE"
              exit 1
            fi
            sleep 5
          done

          echo "Job did not complete in time!"
          exit 1

      - name: View logs on failure
        if: failure()
        run: docker compose logs

      - name: Tear down services
        if: always()
        run: docker compose down
</file>

</files>
</file>

<file path="app/core/auth.py">
from fastapi import Header, HTTPException, status
from app.core.config import settings

def require_api_key(x_api_key: str | None = Header(None)):
    if not x_api_key or x_api_key not in settings.API_KEYS:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Missing or invalid API key"
        )
</file>

<file path="app/core/config.py">
import os

class Settings:
    OLLAMA_HOST: str = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    # Control-plane limits
    CONCURRENCY_LIMIT: int = int(os.getenv("CONCURRENCY_LIMIT", "4"))
    REQUEST_TIMEOUT_S: float = float(os.getenv("REQUEST_TIMEOUT_S", "60"))
    # super basic API keys (move to Redis/DB later)
    API_KEYS: set[str] = set(os.getenv("API_KEYS", "dev-key-123").split(","))

settings = Settings()
</file>

<file path="app/core/logger.py">
import logging, sys

def setup_logger():
    log = logging.getLogger("prodify")
    log.setLevel(logging.INFO)
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(logging.Formatter(
        fmt="%(asctime)s | %(levelname)s | %(message)s"
    ))
    log.handlers = [handler]
    return log

logger = setup_logger()
</file>

<file path="app/core/metrics.py">
from prometheus_client import Counter, Histogram, CollectorRegistry, generate_latest, CONTENT_TYPE_LATEST

registry = CollectorRegistry()
REQUESTS = Counter("requests_total", "Total requests", ["route","model"], registry=registry)
ERRORS   = Counter("errors_total", "Errors", ["route","model"], registry=registry)
LATENCY  = Histogram("latency_seconds", "Request latency", ["route","model"], registry=registry)

def render_metrics():
    return generate_latest(registry), CONTENT_TYPE_LATEST
</file>

<file path="app/models/llm_model.py">
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class LocalLLM:
    def __init__(self, model_name="distilgpt2"):
        print(f"Loading model: {model_name}")
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        self.model.to(self.device)

    def generate(self, prompt: str, max_new_tokens: int = 80):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.8,
            top_p=0.95,
        )
        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return text.strip()
</file>

<file path="app/models/model_registry.py">
# app/models/model_registry.py
from app.models.llm_model import LocalLLM
import threading

class ModelRegistry:
    def __init__(self):
        self.models = {}
        self.initialized = False
        print("üöÄ ModelRegistry initialized but models not loaded yet.")

    def _load_models(self):
        print("üß† Loading models in background thread...")
        try:
            self.models = {
                "distilgpt2": LocalLLM("distilgpt2"),
                "DialoGPT-small": LocalLLM("microsoft/DialoGPT-small"),
                # Comment out TinyLlama until stable (too big for M1)
                # "TinyLlama": LocalLLM("TinyLlama/TinyLlama-1.1B-Chat-v0.6"),
            }
            self.initialized = True
            print(f"‚úÖ Models ready: {list(self.models.keys())}")
        except Exception as e:
            print(f"‚ùå Error loading models: {e}")

    def list_models(self):
        if not self.initialized:
            return []
        return list(self.models.keys())

    def get_model(self, name: str):
        if not self.initialized:
            raise RuntimeError("Models are still loading. Try again later.")
        if name not in self.models:
            raise ValueError(f"Model '{name}' not found.")
        return self.models[name]

    def start_loading(self):
        threading.Thread(target=self._load_models, daemon=True).start()


# Singleton instance
model_registry = ModelRegistry()
</file>

<file path="app/schemas/inference.py">
# app/schemas/inference.py
from pydantic import BaseModel

class InferenceRequest(BaseModel):
    model_name: str ="sentiment"
    input_text: str

    class Config:
        json_schema_extra = {
            "example": {
                "model_name": "sentiment",
                "input_text": "The movie was fantastic!"
            }
        }

class InferenceResponse(BaseModel):
    label: str
    score: float
</file>

<file path="app/static/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Prodify Model API</title>
  <style>
    body { font-family: Inter, sans-serif; background:#fafafa; margin: 40px; }
    .container { max-width: 700px; margin:auto; }
    textarea, select, input { width:100%; padding:8px; margin-top:8px; border-radius:6px; border:1px solid #ccc; }
    button { margin-top:10px; padding:10px 18px; border:none; background:#0077ff; color:white; border-radius:6px; cursor:pointer; }
    button:hover { background:#0059c9; }
    pre { background:white; padding:15px; border-radius:8px; box-shadow:0 0 6px rgba(0,0,0,0.1); white-space:pre-wrap; }
    .loading { animation: blink 1s infinite; }
    @keyframes blink { 50% { opacity: 0.5; } }
  </style>
</head>
<body>
  <div class="container">
    <h1>üöÄ Prodify Inference</h1>
    <label>Model:</label>
    <select id="model"></select>
    <label>Prompt:</label>
    <textarea id="prompt">Once upon a time in Bangalore,</textarea>
    <button onclick="generate()">Generate</button>
    <pre id="output"></pre>
  </div>

  <script>
    async function loadModels() {
      const res = await fetch("/generate/models");
      const data = await res.json();
      const select = document.getElementById("model");
      data.available_models.forEach(m => {
        const opt = document.createElement("option");
        opt.value = m;
        opt.textContent = m;
        select.appendChild(opt);
      });
    }

    async function generate() {
      const model = document.getElementById("model").value;
      const prompt = document.getElementById("prompt").value;
      const output = document.getElementById("output");
      output.textContent = "‚è≥ Generating...";
      output.classList.add("loading");

      const res = await fetch("/generate/", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ model, prompt, max_tokens: 80 })
      });

      const data = await res.json();
      output.classList.remove("loading");
      output.textContent = data.generated_text || JSON.stringify(data, null, 2);
    }

    loadModels();
  </script>
</body>
</html>
</file>

<file path="frontend/public/vite.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>
</file>

<file path="frontend/src/assets/react.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="35.93" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 228"><path fill="#00D8FF" d="M210.483 73.824a171.49 171.49 0 0 0-8.24-2.597c.465-1.9.893-3.777 1.273-5.621c6.238-30.281 2.16-54.676-11.769-62.708c-13.355-7.7-35.196.329-57.254 19.526a171.23 171.23 0 0 0-6.375 5.848a155.866 155.866 0 0 0-4.241-3.917C100.759 3.829 77.587-4.822 63.673 3.233C50.33 10.957 46.379 33.89 51.995 62.588a170.974 170.974 0 0 0 1.892 8.48c-3.28.932-6.445 1.924-9.474 2.98C17.309 83.498 0 98.307 0 113.668c0 15.865 18.582 31.778 46.812 41.427a145.52 145.52 0 0 0 6.921 2.165a167.467 167.467 0 0 0-2.01 9.138c-5.354 28.2-1.173 50.591 12.134 58.266c13.744 7.926 36.812-.22 59.273-19.855a145.567 145.567 0 0 0 5.342-4.923a168.064 168.064 0 0 0 6.92 6.314c21.758 18.722 43.246 26.282 56.54 18.586c13.731-7.949 18.194-32.003 12.4-61.268a145.016 145.016 0 0 0-1.535-6.842c1.62-.48 3.21-.974 4.76-1.488c29.348-9.723 48.443-25.443 48.443-41.52c0-15.417-17.868-30.326-45.517-39.844Zm-6.365 70.984c-1.4.463-2.836.91-4.3 1.345c-3.24-10.257-7.612-21.163-12.963-32.432c5.106-11 9.31-21.767 12.459-31.957c2.619.758 5.16 1.557 7.61 2.4c23.69 8.156 38.14 20.213 38.14 29.504c0 9.896-15.606 22.743-40.946 31.14Zm-10.514 20.834c2.562 12.94 2.927 24.64 1.23 33.787c-1.524 8.219-4.59 13.698-8.382 15.893c-8.067 4.67-25.32-1.4-43.927-17.412a156.726 156.726 0 0 1-6.437-5.87c7.214-7.889 14.423-17.06 21.459-27.246c12.376-1.098 24.068-2.894 34.671-5.345a134.17 134.17 0 0 1 1.386 6.193ZM87.276 214.515c-7.882 2.783-14.16 2.863-17.955.675c-8.075-4.657-11.432-22.636-6.853-46.752a156.923 156.923 0 0 1 1.869-8.499c10.486 2.32 22.093 3.988 34.498 4.994c7.084 9.967 14.501 19.128 21.976 27.15a134.668 134.668 0 0 1-4.877 4.492c-9.933 8.682-19.886 14.842-28.658 17.94ZM50.35 144.747c-12.483-4.267-22.792-9.812-29.858-15.863c-6.35-5.437-9.555-10.836-9.555-15.216c0-9.322 13.897-21.212 37.076-29.293c2.813-.98 5.757-1.905 8.812-2.773c3.204 10.42 7.406 21.315 12.477 32.332c-5.137 11.18-9.399 22.249-12.634 32.792a134.718 134.718 0 0 1-6.318-1.979Zm12.378-84.26c-4.811-24.587-1.616-43.134 6.425-47.789c8.564-4.958 27.502 2.111 47.463 19.835a144.318 144.318 0 0 1 3.841 3.545c-7.438 7.987-14.787 17.08-21.808 26.988c-12.04 1.116-23.565 2.908-34.161 5.309a160.342 160.342 0 0 1-1.76-7.887Zm110.427 27.268a347.8 347.8 0 0 0-7.785-12.803c8.168 1.033 15.994 2.404 23.343 4.08c-2.206 7.072-4.956 14.465-8.193 22.045a381.151 381.151 0 0 0-7.365-13.322Zm-45.032-43.861c5.044 5.465 10.096 11.566 15.065 18.186a322.04 322.04 0 0 0-30.257-.006c4.974-6.559 10.069-12.652 15.192-18.18ZM82.802 87.83a323.167 323.167 0 0 0-7.227 13.238c-3.184-7.553-5.909-14.98-8.134-22.152c7.304-1.634 15.093-2.97 23.209-3.984a321.524 321.524 0 0 0-7.848 12.897Zm8.081 65.352c-8.385-.936-16.291-2.203-23.593-3.793c2.26-7.3 5.045-14.885 8.298-22.6a321.187 321.187 0 0 0 7.257 13.246c2.594 4.48 5.28 8.868 8.038 13.147Zm37.542 31.03c-5.184-5.592-10.354-11.779-15.403-18.433c4.902.192 9.899.29 14.978.29c5.218 0 10.376-.117 15.453-.343c-4.985 6.774-10.018 12.97-15.028 18.486Zm52.198-57.817c3.422 7.8 6.306 15.345 8.596 22.52c-7.422 1.694-15.436 3.058-23.88 4.071a382.417 382.417 0 0 0 7.859-13.026a347.403 347.403 0 0 0 7.425-13.565Zm-16.898 8.101a358.557 358.557 0 0 1-12.281 19.815a329.4 329.4 0 0 1-23.444.823c-7.967 0-15.716-.248-23.178-.732a310.202 310.202 0 0 1-12.513-19.846h.001a307.41 307.41 0 0 1-10.923-20.627a310.278 310.278 0 0 1 10.89-20.637l-.001.001a307.318 307.318 0 0 1 12.413-19.761c7.613-.576 15.42-.876 23.31-.876H128c7.926 0 15.743.303 23.354.883a329.357 329.357 0 0 1 12.335 19.695a358.489 358.489 0 0 1 11.036 20.54a329.472 329.472 0 0 1-11 20.722Zm22.56-122.124c8.572 4.944 11.906 24.881 6.52 51.026c-.344 1.668-.73 3.367-1.15 5.09c-10.622-2.452-22.155-4.275-34.23-5.408c-7.034-10.017-14.323-19.124-21.64-27.008a160.789 160.789 0 0 1 5.888-5.4c18.9-16.447 36.564-22.941 44.612-18.3ZM128 90.808c12.625 0 22.86 10.235 22.86 22.86s-10.235 22.86-22.86 22.86s-22.86-10.235-22.86-22.86s10.235-22.86 22.86-22.86Z"></path></svg>
</file>

<file path="frontend/src/components/ui/button.tsx">
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"

import { cn } from "@/lib/utils"

const buttonVariants = cva(
  "inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-white hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost:
          "hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2 has-[>svg]:px-3",
        sm: "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
        lg: "h-10 rounded-md px-6 has-[>svg]:px-4",
        icon: "size-9",
        "icon-sm": "size-8",
        "icon-lg": "size-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)

function Button({
  className,
  variant,
  size,
  asChild = false,
  ...props
}: React.ComponentProps<"button"> &
  VariantProps<typeof buttonVariants> & {
    asChild?: boolean
  }) {
  const Comp = asChild ? Slot : "button"

  return (
    <Comp
      data-slot="button"
      className={cn(buttonVariants({ variant, size, className }))}
      {...props}
    />
  )
}

export { Button, buttonVariants }
</file>

<file path="frontend/src/components/ui/card.tsx">
import * as React from "react"

import { cn } from "@/lib/utils"

function Card({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card"
      className={cn(
        "bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm",
        className
      )}
      {...props}
    />
  )
}

function CardHeader({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-header"
      className={cn(
        "@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-2 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6",
        className
      )}
      {...props}
    />
  )
}

function CardTitle({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-title"
      className={cn("leading-none font-semibold", className)}
      {...props}
    />
  )
}

function CardDescription({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-description"
      className={cn("text-muted-foreground text-sm", className)}
      {...props}
    />
  )
}

function CardAction({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-action"
      className={cn(
        "col-start-2 row-span-2 row-start-1 self-start justify-self-end",
        className
      )}
      {...props}
    />
  )
}

function CardContent({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-content"
      className={cn("px-6", className)}
      {...props}
    />
  )
}

function CardFooter({ className, ...props }: React.ComponentProps<"div">) {
  return (
    <div
      data-slot="card-footer"
      className={cn("flex items-center px-6 [.border-t]:pt-6", className)}
      {...props}
    />
  )
}

export {
  Card,
  CardHeader,
  CardFooter,
  CardTitle,
  CardAction,
  CardDescription,
  CardContent,
}
</file>

<file path="frontend/src/components/ui/label.tsx">
"use client"

import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"

import { cn } from "@/lib/utils"

function Label({
  className,
  ...props
}: React.ComponentProps<typeof LabelPrimitive.Root>) {
  return (
    <LabelPrimitive.Root
      data-slot="label"
      className={cn(
        "flex items-center gap-2 text-sm leading-none font-medium select-none group-data-[disabled=true]:pointer-events-none group-data-[disabled=true]:opacity-50 peer-disabled:cursor-not-allowed peer-disabled:opacity-50",
        className
      )}
      {...props}
    />
  )
}

export { Label }
</file>

<file path="frontend/src/components/ui/select.tsx">
import * as React from "react"
import * as SelectPrimitive from "@radix-ui/react-select"
import { CheckIcon, ChevronDownIcon, ChevronUpIcon } from "lucide-react"

import { cn } from "@/lib/utils"

function Select({
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Root>) {
  return <SelectPrimitive.Root data-slot="select" {...props} />
}

function SelectGroup({
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Group>) {
  return <SelectPrimitive.Group data-slot="select-group" {...props} />
}

function SelectValue({
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Value>) {
  return <SelectPrimitive.Value data-slot="select-value" {...props} />
}

function SelectTrigger({
  className,
  size = "default",
  children,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Trigger> & {
  size?: "sm" | "default"
}) {
  return (
    <SelectPrimitive.Trigger
      data-slot="select-trigger"
      data-size={size}
      className={cn(
        "border-input data-[placeholder]:text-muted-foreground [&_svg:not([class*='text-'])]:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 dark:hover:bg-input/50 flex w-fit items-center justify-between gap-2 rounded-md border bg-transparent px-3 py-2 text-sm whitespace-nowrap shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 data-[size=default]:h-9 data-[size=sm]:h-8 *:data-[slot=select-value]:line-clamp-1 *:data-[slot=select-value]:flex *:data-[slot=select-value]:items-center *:data-[slot=select-value]:gap-2 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",
        className
      )}
      {...props}
    >
      {children}
      <SelectPrimitive.Icon asChild>
        <ChevronDownIcon className="size-4 opacity-50" />
      </SelectPrimitive.Icon>
    </SelectPrimitive.Trigger>
  )
}

function SelectContent({
  className,
  children,
  position = "popper",
  align = "center",
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Content>) {
  return (
    <SelectPrimitive.Portal>
      <SelectPrimitive.Content
        data-slot="select-content"
        className={cn(
          "bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 relative z-50 max-h-(--radix-select-content-available-height) min-w-[8rem] origin-(--radix-select-content-transform-origin) overflow-x-hidden overflow-y-auto rounded-md border shadow-md",
          position === "popper" &&
            "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
          className
        )}
        position={position}
        align={align}
        {...props}
      >
        <SelectScrollUpButton />
        <SelectPrimitive.Viewport
          className={cn(
            "p-1",
            position === "popper" &&
              "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)] scroll-my-1"
          )}
        >
          {children}
        </SelectPrimitive.Viewport>
        <SelectScrollDownButton />
      </SelectPrimitive.Content>
    </SelectPrimitive.Portal>
  )
}

function SelectLabel({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Label>) {
  return (
    <SelectPrimitive.Label
      data-slot="select-label"
      className={cn("text-muted-foreground px-2 py-1.5 text-xs", className)}
      {...props}
    />
  )
}

function SelectItem({
  className,
  children,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Item>) {
  return (
    <SelectPrimitive.Item
      data-slot="select-item"
      className={cn(
        "focus:bg-accent focus:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground relative flex w-full cursor-default items-center gap-2 rounded-sm py-1.5 pr-8 pl-2 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4 *:[span]:last:flex *:[span]:last:items-center *:[span]:last:gap-2",
        className
      )}
      {...props}
    >
      <span className="absolute right-2 flex size-3.5 items-center justify-center">
        <SelectPrimitive.ItemIndicator>
          <CheckIcon className="size-4" />
        </SelectPrimitive.ItemIndicator>
      </span>
      <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
    </SelectPrimitive.Item>
  )
}

function SelectSeparator({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.Separator>) {
  return (
    <SelectPrimitive.Separator
      data-slot="select-separator"
      className={cn("bg-border pointer-events-none -mx-1 my-1 h-px", className)}
      {...props}
    />
  )
}

function SelectScrollUpButton({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.ScrollUpButton>) {
  return (
    <SelectPrimitive.ScrollUpButton
      data-slot="select-scroll-up-button"
      className={cn(
        "flex cursor-default items-center justify-center py-1",
        className
      )}
      {...props}
    >
      <ChevronUpIcon className="size-4" />
    </SelectPrimitive.ScrollUpButton>
  )
}

function SelectScrollDownButton({
  className,
  ...props
}: React.ComponentProps<typeof SelectPrimitive.ScrollDownButton>) {
  return (
    <SelectPrimitive.ScrollDownButton
      data-slot="select-scroll-down-button"
      className={cn(
        "flex cursor-default items-center justify-center py-1",
        className
      )}
      {...props}
    >
      <ChevronDownIcon className="size-4" />
    </SelectPrimitive.ScrollDownButton>
  )
}

export {
  Select,
  SelectContent,
  SelectGroup,
  SelectItem,
  SelectLabel,
  SelectScrollDownButton,
  SelectScrollUpButton,
  SelectSeparator,
  SelectTrigger,
  SelectValue,
}
</file>

<file path="frontend/src/components/ui/slider.tsx">
import * as React from "react"
import * as SliderPrimitive from "@radix-ui/react-slider"

import { cn } from "@/lib/utils"

function Slider({
  className,
  defaultValue,
  value,
  min = 0,
  max = 100,
  ...props
}: React.ComponentProps<typeof SliderPrimitive.Root>) {
  const _values = React.useMemo(
    () =>
      Array.isArray(value)
        ? value
        : Array.isArray(defaultValue)
          ? defaultValue
          : [min, max],
    [value, defaultValue, min, max]
  )

  return (
    <SliderPrimitive.Root
      data-slot="slider"
      defaultValue={defaultValue}
      value={value}
      min={min}
      max={max}
      className={cn(
        "relative flex w-full touch-none items-center select-none data-[disabled]:opacity-50 data-[orientation=vertical]:h-full data-[orientation=vertical]:min-h-44 data-[orientation=vertical]:w-auto data-[orientation=vertical]:flex-col",
        className
      )}
      {...props}
    >
      <SliderPrimitive.Track
        data-slot="slider-track"
        className={cn(
          "bg-muted relative grow overflow-hidden rounded-full data-[orientation=horizontal]:h-1.5 data-[orientation=horizontal]:w-full data-[orientation=vertical]:h-full data-[orientation=vertical]:w-1.5"
        )}
      >
        <SliderPrimitive.Range
          data-slot="slider-range"
          className={cn(
            "bg-primary absolute data-[orientation=horizontal]:h-full data-[orientation=vertical]:w-full"
          )}
        />
      </SliderPrimitive.Track>
      {Array.from({ length: _values.length }, (_, index) => (
        <SliderPrimitive.Thumb
          data-slot="slider-thumb"
          key={index}
          className="border-primary ring-ring/50 block size-4 shrink-0 rounded-full border bg-white shadow-sm transition-[color,box-shadow] hover:ring-4 focus-visible:ring-4 focus-visible:outline-hidden disabled:pointer-events-none disabled:opacity-50"
        />
      ))}
    </SliderPrimitive.Root>
  )
}

export { Slider }
</file>

<file path="frontend/src/components/ui/textarea.tsx">
import * as React from "react"

import { cn } from "@/lib/utils"

function Textarea({ className, ...props }: React.ComponentProps<"textarea">) {
  return (
    <textarea
      data-slot="textarea"
      className={cn(
        "border-input placeholder:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 flex field-sizing-content min-h-16 w-full rounded-md border bg-transparent px-3 py-2 text-base shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",
        className
      )}
      {...props}
    />
  )
}

export { Textarea }
</file>

<file path="frontend/src/lib/utils.ts">
import { clsx, type ClassValue } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
</file>

<file path="frontend/src/App.css">
#root {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em #646cffaa);
}
.logo.react:hover {
  filter: drop-shadow(0 0 2em #61dafbaa);
}

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: #888;
}
</file>

<file path="frontend/src/index.css">
@import "tailwindcss";
@import "tw-animate-css";

@custom-variant dark (&:is(.dark *));

@theme inline {
  --radius-sm: calc(var(--radius) - 4px);
  --radius-md: calc(var(--radius) - 2px);
  --radius-lg: var(--radius);
  --radius-xl: calc(var(--radius) + 4px);
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --color-card: var(--card);
  --color-card-foreground: var(--card-foreground);
  --color-popover: var(--popover);
  --color-popover-foreground: var(--popover-foreground);
  --color-primary: var(--primary);
  --color-primary-foreground: var(--primary-foreground);
  --color-secondary: var(--secondary);
  --color-secondary-foreground: var(--secondary-foreground);
  --color-muted: var(--muted);
  --color-muted-foreground: var(--muted-foreground);
  --color-accent: var(--accent);
  --color-accent-foreground: var(--accent-foreground);
  --color-destructive: var(--destructive);
  --color-border: var(--border);
  --color-input: var(--input);
  --color-ring: var(--ring);
  --color-chart-1: var(--chart-1);
  --color-chart-2: var(--chart-2);
  --color-chart-3: var(--chart-3);
  --color-chart-4: var(--chart-4);
  --color-chart-5: var(--chart-5);
  --color-sidebar: var(--sidebar);
  --color-sidebar-foreground: var(--sidebar-foreground);
  --color-sidebar-primary: var(--sidebar-primary);
  --color-sidebar-primary-foreground: var(--sidebar-primary-foreground);
  --color-sidebar-accent: var(--sidebar-accent);
  --color-sidebar-accent-foreground: var(--sidebar-accent-foreground);
  --color-sidebar-border: var(--sidebar-border);
  --color-sidebar-ring: var(--sidebar-ring);
}

:root {
  --radius: 0.625rem;
  --background: oklch(1 0 0);
  --foreground: oklch(0.145 0 0);
  --card: oklch(1 0 0);
  --card-foreground: oklch(0.145 0 0);
  --popover: oklch(1 0 0);
  --popover-foreground: oklch(0.145 0 0);
  --primary: oklch(0.205 0 0);
  --primary-foreground: oklch(0.985 0 0);
  --secondary: oklch(0.97 0 0);
  --secondary-foreground: oklch(0.205 0 0);
  --muted: oklch(0.97 0 0);
  --muted-foreground: oklch(0.556 0 0);
  --accent: oklch(0.97 0 0);
  --accent-foreground: oklch(0.205 0 0);
  --destructive: oklch(0.577 0.245 27.325);
  --border: oklch(0.922 0 0);
  --input: oklch(0.922 0 0);
  --ring: oklch(0.708 0 0);
  --chart-1: oklch(0.646 0.222 41.116);
  --chart-2: oklch(0.6 0.118 184.704);
  --chart-3: oklch(0.398 0.07 227.392);
  --chart-4: oklch(0.828 0.189 84.429);
  --chart-5: oklch(0.769 0.188 70.08);
  --sidebar: oklch(0.985 0 0);
  --sidebar-foreground: oklch(0.145 0 0);
  --sidebar-primary: oklch(0.205 0 0);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.97 0 0);
  --sidebar-accent-foreground: oklch(0.205 0 0);
  --sidebar-border: oklch(0.922 0 0);
  --sidebar-ring: oklch(0.708 0 0);
}

.dark {
  --background: oklch(0.145 0 0);
  --foreground: oklch(0.985 0 0);
  --card: oklch(0.205 0 0);
  --card-foreground: oklch(0.985 0 0);
  --popover: oklch(0.205 0 0);
  --popover-foreground: oklch(0.985 0 0);
  --primary: oklch(0.922 0 0);
  --primary-foreground: oklch(0.205 0 0);
  --secondary: oklch(0.269 0 0);
  --secondary-foreground: oklch(0.985 0 0);
  --muted: oklch(0.269 0 0);
  --muted-foreground: oklch(0.708 0 0);
  --accent: oklch(0.269 0 0);
  --accent-foreground: oklch(0.985 0 0);
  --destructive: oklch(0.704 0.191 22.216);
  --border: oklch(1 0 0 / 10%);
  --input: oklch(1 0 0 / 15%);
  --ring: oklch(0.556 0 0);
  --chart-1: oklch(0.488 0.243 264.376);
  --chart-2: oklch(0.696 0.17 162.48);
  --chart-3: oklch(0.769 0.188 70.08);
  --chart-4: oklch(0.627 0.265 303.9);
  --chart-5: oklch(0.645 0.246 16.439);
  --sidebar: oklch(0.205 0 0);
  --sidebar-foreground: oklch(0.985 0 0);
  --sidebar-primary: oklch(0.488 0.243 264.376);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.269 0 0);
  --sidebar-accent-foreground: oklch(0.985 0 0);
  --sidebar-border: oklch(1 0 0 / 10%);
  --sidebar-ring: oklch(0.556 0 0);
}

@layer base {
  * {
    @apply border-border outline-ring/50;
  }
  body {
    @apply bg-background text-foreground;
  }
}
</file>

<file path="frontend/src/main.tsx">
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.tsx'

createRoot(document.getElementById('root')!).render(
  <StrictMode>
    <App />
  </StrictMode>,
)
</file>

<file path="frontend/components.json">
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "new-york",
  "rsc": false,
  "tsx": true,
  "tailwind": {
    "config": "",
    "css": "src/index.css",
    "baseColor": "neutral",
    "cssVariables": true,
    "prefix": ""
  },
  "iconLibrary": "lucide",
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils",
    "ui": "@/components/ui",
    "lib": "@/lib",
    "hooks": "@/hooks"
  },
  "registries": {}
}
</file>

<file path="frontend/Dockerfile.frontend">
# Stage 1: Build the React application
FROM node:20-alpine AS build

WORKDIR /app

# Copy package files and install dependencies
COPY package.json package-lock.json ./
RUN npm install

# Copy the rest of the application source code
COPY . .

# Build the application
RUN npm run build

# Stage 2: Serve the application with Nginx
FROM nginx:1.25-alpine AS final

# Copy the built assets from the build stage
COPY --from=build /app/dist /usr/share/nginx/html

# Copy the custom Nginx configuration
# We will create this file next
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port 80
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]
</file>

<file path="frontend/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>frontend</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="frontend/package.json">
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "@radix-ui/react-label": "^2.1.7",
    "@radix-ui/react-select": "^2.2.6",
    "@radix-ui/react-slider": "^1.3.6",
    "@radix-ui/react-slot": "^1.2.3",
    "@tailwindcss/vite": "^4.1.14",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "lucide-react": "^0.545.0",
    "react": "^19.1.1",
    "react-dom": "^19.1.1",
    "tailwind-merge": "^3.3.1",
    "tailwindcss": "^4.1.14"
  },
  "devDependencies": {
    "@eslint/js": "^9.36.0",
    "@types/node": "^24.7.2",
    "@types/react": "^19.1.16",
    "@types/react-dom": "^19.1.9",
    "@vitejs/plugin-react": "^5.0.4",
    "eslint": "^9.36.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.22",
    "globals": "^16.4.0",
    "tw-animate-css": "^1.4.0",
    "typescript": "~5.9.3",
    "typescript-eslint": "^8.45.0",
    "vite": "^7.1.7"
  }
}
</file>

<file path="frontend/README.md">
# React + TypeScript + Vite

This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.

Currently, two official plugins are available:

- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) (or [oxc](https://oxc.rs) when used in [rolldown-vite](https://vite.dev/guide/rolldown)) for Fast Refresh
- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh

## React Compiler

The React Compiler is not enabled on this template because of its impact on dev & build performances. To add it, see [this documentation](https://react.dev/learn/react-compiler/installation).

## Expanding the ESLint configuration

If you are developing a production application, we recommend updating the configuration to enable type-aware lint rules:

```js
export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      // Other configs...

      // Remove tseslint.configs.recommended and replace with this
      tseslint.configs.recommendedTypeChecked,
      // Alternatively, use this for stricter rules
      tseslint.configs.strictTypeChecked,
      // Optionally, add this for stylistic rules
      tseslint.configs.stylisticTypeChecked,

      // Other configs...
    ],
    languageOptions: {
      parserOptions: {
        project: ['./tsconfig.node.json', './tsconfig.app.json'],
        tsconfigRootDir: import.meta.dirname,
      },
      // other options...
    },
  },
])
```

You can also install [eslint-plugin-react-x](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-x) and [eslint-plugin-react-dom](https://github.com/Rel1cx/eslint-react/tree/main/packages/plugins/eslint-plugin-react-dom) for React-specific lint rules:

```js
// eslint.config.js
import reactX from 'eslint-plugin-react-x'
import reactDom from 'eslint-plugin-react-dom'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      // Other configs...
      // Enable lint rules for React
      reactX.configs['recommended-typescript'],
      // Enable lint rules for React DOM
      reactDom.configs.recommended,
    ],
    languageOptions: {
      parserOptions: {
        project: ['./tsconfig.node.json', './tsconfig.app.json'],
        tsconfigRootDir: import.meta.dirname,
      },
      // other options...
    },
  },
])
```
</file>

<file path="frontend/tsconfig.app.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "types": ["vite/client"],
    "skipLibCheck": true,
    "baseUrl": ".",
    "paths": {
      "@/*": [
        "./src/*"
      ]
    },

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}
</file>

<file path="frontend/tsconfig.json">
{
  "files": [],
  "references": [
    {
      "path": "./tsconfig.app.json"
    },
    {
      "path": "./tsconfig.node.json"
    }
  ],
  "compilerOptions": {
    "baseUrl": ".",
    "paths": {
      "@/*": ["./src/*"]
    }
  }
}
</file>

<file path="frontend/tsconfig.node.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "types": ["node"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="tests/test_api_generate.py">
from fastapi.testclient import TestClient
from unittest.mock import MagicMock

from app.main import app
from app.core.config import settings

client = TestClient(app)

# Get a valid key from the settings
VALID_API_KEY = list(settings.API_KEYS)[0]

def test_enqueue_generate_success(monkeypatch):
    """Test successful enqueue of a generation job."""
    # Mock the celery task's delay method
    mock_delay = MagicMock()
    mock_delay.return_value = MagicMock(id="test-job-123")
    monkeypatch.setattr("app.worker.generate_text_task.delay", mock_delay)

    payload = {
        "model": "qwen3:0.6b",
        "prompt": "Why is the sky blue?",
        "max_tokens": 100,
    }

    response = client.post(
        "/generate",
        headers={"x-api-key": VALID_API_KEY},
        json=payload,
    )

    assert response.status_code == 200
    assert response.json() == {"job_id": "test-job-123", "status": "queued"}

    # Verify that the task was called correctly
    mock_delay.assert_called_once_with(
        payload["model"],
        payload["prompt"],
        payload["max_tokens"],
        0.7,  # temperature defaults to 0.7 in the pydantic model
    )

def test_enqueue_generate_no_api_key():
    """Test request without an API key."""
    response = client.post("/generate", json={})
    assert response.status_code == 401
    assert response.json() == {"detail": "Missing or invalid API key"}

def test_enqueue_generate_invalid_api_key():
    """Test request with an invalid API key."""
    response = client.post(
        "/generate",
        headers={"x-api-key": "invalid-key"},
        json={},
    )
    assert response.status_code == 401
    assert response.json() == {"detail": "Missing or invalid API key"}
</file>

<file path="tests/test_api_models.py">
import pytest

from fastapi.testclient import TestClient

import httpx



from app.main import app





def test_list_models_success(monkeypatch):

    """Test successful listing of models by monkeypatching the client method."""



    async def mock_list_models(*args, **kwargs):

        return ["qwen3:0.6b", "mistral:latest"]



    monkeypatch.setattr("app.services.ollama_client.OllamaClient.list_models", mock_list_models)



    with TestClient(app) as client:

        response = client.get("/models")



    assert response.status_code == 200

    assert response.json() == {"models": ["qwen3:0.6b", "mistral:latest"]}





def test_list_models_ollama_error(monkeypatch):

    """Test an error case by having the monkeypatched method raise an exception."""



    async def mock_list_models_error(*args, **kwargs):

        raise httpx.RequestError("Mocked network error")



    monkeypatch.setattr("app.services.ollama_client.OllamaClient.list_models", mock_list_models_error)



    with TestClient(app) as client:

        response = client.get("/models")



    assert response.status_code == 500

    assert "Mocked network error" in response.json()["detail"]
</file>

<file path="tests/test_main.py">
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_healthz():
    """Test the health check endpoint."""
    response = client.get("/healthz")
    assert response.status_code == 200
    assert response.json() == {"ok": True}
</file>

<file path="Dockerfile">
# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Set environment variables
# Prevents Python from writing pyc files to disc
ENV PYTHONDONTWRITEBYTECODE 1
# Ensures Python output is sent straight to the terminal without buffering
ENV PYTHONUNBUFFERED 1
# Set the python path to include the project root
ENV PYTHONPATH=.

# Set the working directory in the container
WORKDIR /app

# Install dependencies
# Copy the requirements file first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code into the container
COPY ./app /app/app

# The command to run the application will be specified in docker-compose.yml
</file>

<file path="app/api/routes_generate.py">
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from app.core.auth import require_api_key
from app.worker import generate_text_task

router = APIRouter()

class GenerateRequest(BaseModel):
    model: str = "qwen3:0.6b"
    prompt: str
    max_tokens: int | None = None
    temperature: float | None = 0.7
    top_p: float | None = 0.9
    stream: bool = False

@router.post("", dependencies=[Depends(require_api_key)])
async def enqueue_generate(req: GenerateRequest):
    """Enqueue text generation job"""
    task = generate_text_task.delay(req.model, req.prompt, req.max_tokens, req.temperature)
    return {"job_id": task.id, "status": "queued"}

# @router.post("/", dependencies=[Depends(require_api_key)])
# async def generate(req: GenerateRequest):
#     options = {}
#     if req.max_tokens is not None: options["num_predict"] = req.max_tokens
#     if req.temperature is not None: options["temperature"] = req.temperature
#     if req.top_p is not None: options["top_p"] = req.top_p

#     if not req.stream:
#         try:
#             data = await ollama.generate_once(req.model, req.prompt, options)
#             return JSONResponse(data)
#         except Exception as e:
#             raise HTTPException(status_code=500, detail=str(e))

#     async def gen():
#         try:
#             async for line in ollama.stream_generate(req.model, req.prompt, options):
#                 yield line + "\n"
#         except Exception as e:
#             yield json.dumps({"error": str(e)}) + "\n"

#     return StreamingResponse(gen(), media_type="application/x-ndjson")
</file>

<file path="app/api/routes_models.py">
# app/api/routes_models.py
from fastapi import APIRouter, HTTPException, Request

router = APIRouter()

@router.get("")  
async def list_models(request: Request):
    try:
        client = request.app.state.ollama  # ‚úÖ use app.state
        models = await client.list_models()
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="app/api/routes_status.py">
# app/api/routes_status.py
from fastapi import APIRouter
from celery.result import AsyncResult
from app.worker import celery_app

router = APIRouter()

@router.get("/{job_id}")
async def get_status(job_id: str):
    """Return Celery job status and result."""
    result = AsyncResult(job_id, app=celery_app)

    if result.state == "PENDING":
        return {"status": "queued"}
    elif result.state == "STARTED":
        return {"status": "processing"}
    elif result.state == "SUCCESS":
        return {
            "status": "completed",
            "result": result.result,  # {'model': '...', 'text': '...'}
        }
    elif result.state == "FAILURE":
        return {"status": "failed", "error": str(result.result)}
    else:
        return {"status": result.state.lower()}
</file>

<file path="app/main.py">
# app/main.py

from fastapi import FastAPI, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from starlette.middleware.base import BaseHTTPMiddleware
import asyncio, time
from app.api import routes_generate, routes_models, routes_status
from app.core.config import settings
from app.core.logger import logger
from app.core.metrics import render_metrics, LATENCY, REQUESTS, ERRORS
from app.services.ollama_client import OllamaClient  # ‚úÖ use the class

app = FastAPI(title="Prodify Gateway", version="3.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

sem = asyncio.Semaphore(settings.CONCURRENCY_LIMIT)

class MetricsMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        if request.url.path in ("/healthz", "/readyz", "/metrics", "/static/index.html"):
            return await call_next(request)

        model = "unknown"
        if request.headers.get("content-type") == "application/json":
            try:
                body_bytes = await request.body()
                body = body_bytes.decode("utf-8")
                # avoid consuming body: rebuild scope receive
                async def receive():
                    return {"type": "http.request", "body": body_bytes}
                request = Request(request.scope, receive)
                import json
                model = json.loads(body).get("model", "unknown")
            except:
                pass

        route = request.url.path
        start = time.perf_counter()
        resp = None
        try:
            async with sem:
                resp = await call_next(request)
            return resp
        finally:
            if resp is not None:
                LATENCY.labels(route=route, model=model).observe(time.perf_counter() - start)
                REQUESTS.labels(route=route, model=model).inc()

app.add_middleware(MetricsMiddleware)

# Routers
app.include_router(routes_models.router, prefix="/models", tags=["Models"])
app.include_router(routes_generate.router, prefix="/generate", tags=["Generate"])
app.include_router(routes_status.router, prefix="/status", tags=["Status"])

# Static
app.mount("/static", StaticFiles(directory="app/static"), name="static")

@app.on_event("startup")
async def on_startup():
    logger.info("üöÄ Gateway starting...")
    # ‚úÖ create a client for the API process and keep it in app.state
    app.state.ollama = OllamaClient()
    try:
        models = await app.state.ollama.list_models()
        logger.info(f"‚úÖ Ollama reachable. Models available: {models}")
    except Exception as e:
        logger.error(f"‚ùå Ollama not reachable on startup: {e}")

@app.on_event("shutdown")
async def on_shutdown():
    logger.info("üåô Gateway shutting down.")
    if hasattr(app.state, "ollama"):
        await app.state.ollama.close()

@app.get("/healthz")
async def healthz():
    return {"ok": True}

@app.get("/readyz")
async def readyz():
    try:
        models = await app.state.ollama.list_models()  # ‚úÖ use app.state
        return {"ready": True, "models": len(models)}
    except Exception as e:
        return {"ready": False, "error": str(e)}

@app.get("/metrics")
async def metrics():
    body, ctype = render_metrics()
    return Response(content=body, media_type=ctype)
</file>

<file path="frontend/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local
.env*

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
</file>

<file path="frontend/nginx.conf">
server {
  listen 80;

  # Root directory for static files
  root /usr/share/nginx/html;
  index index.html;

  # Handle client-side routing for React
  location / {
    try_files $uri $uri/ /index.html;
  }

  # Proxy API requests to the backend service
  location ~ ^/(models|generate|status) {
    proxy_pass http://backend:8000;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header x-api-key $http_x_api_key;
  }

  # Optional: Add headers to prevent caching in development
  location ~* \.(?:html|js|css)$ {
    add_header Cache-Control "no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0";
    expires off;
    etag off;
  }
}
</file>

<file path="frontend/vite.config.ts">
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import path from "path"
import tailwindcss from "@tailwindcss/vite"

// https://vite.dev/config/
export default defineConfig({
  plugins: [react(), tailwindcss()],
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
    },
  },
  server: {
    proxy: {
      "/generate": "http://127.0.0.1:8000",
      "/models": "http://127.0.0.1:8000",
      "/status": "http://127.0.0.1:8000",
    },
  },
  
})
</file>

<file path=".env.example">
# API keys for the backend, comma-separated
API_KEYS=your-development-api-key
</file>

<file path=".gitignore">
.venv
__pycache__
.env
dump.rdb
</file>

<file path="README.md">
# üöÄ Prodify AI Inference Gateway

## A Scalable and Robust Asynchronous AI Inference System

This project demonstrates a modern, scalable, and fault-tolerant architecture for serving AI model inferences asynchronously. It's designed to handle concurrent requests efficiently, provide real-time status updates, and integrate seamlessly with various Large Language Models (LLMs) via Ollama.

## ‚ú® Key Features & Benefits

- **Asynchronous Inference:** Leverages Celery and Redis for background task processing, ensuring a responsive user experience and efficient resource utilization.
- **Scalable Architecture:** Built with FastAPI, Celery, and Redis, the system can easily scale horizontally to meet increasing demand for AI inferences.
- **LLM Agnostic:** Integrates with Ollama, allowing for flexible deployment and experimentation with various local Large Language Models (LLMs) like Qwen and Mistral.
- **Modern Frontend:** A responsive and interactive user interface developed with React (Vite + Shadcn) for seamless user interaction and real-time feedback.
- **Secure API Access:** Implements API key-based authentication to secure inference endpoints.
- **Observability:** Includes Prometheus metrics for monitoring system performance and health.
- **Containerized Deployment:** Docker and Docker Compose configurations for easy setup, deployment, and environment consistency.

## üß† Architecture Overview

The system is composed of several interconnected services, each playing a crucial role in the inference pipeline:

```
+-------------------+       +-------------------+       +-------------------+
| Frontend (React)  | <---> | FastAPI Gateway   | <---> | Redis (Broker)    |
| (User Interface)  |       | (API, Auth, Queue)|       | (Task Queue)      |
+-------------------+       +-------------------+       +-------------------+
         ‚ñ≤                           ‚îÇ                           ‚îÇ
         ‚îÇ                           ‚îÇ                           ‚ñº
         ‚îÇ                     (Enqueues Task)             +-------------------+
         ‚îÇ                                                 | Celery Worker     |
         ‚îÇ                                                 | (Task Execution)  |
         ‚îÇ                                                 +-------------------+
         ‚îÇ                                                           ‚îÇ
         ‚îÇ                                                           ‚ñº
         ‚îÇ                                                     +-------------------+
         +-----------------------------------------------------| Ollama (LLM)      |
           (Polls for Results)                                 | (Model Inference) |
                                                               +-------------------+
```

### Component Breakdown:

- **Frontend (React + Shadcn):** A single-page application (SPA) built with React, Vite, and Shadcn UI components. It provides an intuitive interface for users to submit prompts, select LLMs, and view inference results in real-time through polling.
- **FastAPI Gateway:** The central API layer built with FastAPI. It handles:
  - API key authentication.
  - Routing for inference requests (`/generate`), model listing (`/models`), and status checks (`/status/{job_id}`).
  - Enqueuing inference tasks to Celery.
  - Exposing Prometheus metrics for monitoring.
- **Redis:** Serves as both the message broker for Celery (managing the task queue) and the backend for storing Celery task results.
- **Celery Worker:** A distributed task queue worker that consumes tasks from Redis. It orchestrates the actual LLM inference by interacting with the Ollama service.
- **Ollama:** A powerful tool for running Large Language Models locally. The Celery worker sends prompts to Ollama, which then performs the inference using the selected LLM (e.g., Qwen, Mistral).

## ‚öôÔ∏è Tech Stack

| Layer                | Technology                         | Role                                                             |
| :------------------- | :--------------------------------- | :--------------------------------------------------------------- |
| **Frontend**         | React, Vite, Shadcn UI, TypeScript | Interactive UI for prompt submission and result display          |
| **Backend API**      | FastAPI, Python                    | High-performance API gateway, authentication, task enqueuing     |
| **Task Queue**       | Celery, Redis                      | Asynchronous task management, message brokering, result storage  |
| **LLM Inference**    | Ollama                             | Local LLM serving (supports various models like Qwen, Mistral)   |
| **Containerization** | Docker, Docker Compose             | Environment consistency, simplified deployment and orchestration |
| **Monitoring**       | Prometheus Client (Python)         | Capturing and exposing application metrics                       |

## üöÄ Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites

- Docker and Docker Compose
- Git

### Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/sachinkumaryadav/deploy-ai-fastapi.git
    cd deploy-ai-fastapi
    ```

2.  **Configure Environment Variables:**
    Copy the example environment file and update it with your API keys.

    ```bash
    cp .env.example .env
    # Open .env and set your API_KEYS (e.g., API_KEYS="dev-key-123,prod-key-456")
    ```

3.  **Start the Services:**
    Use Docker Compose to build and run all services. This will start Redis, Ollama, FastAPI backend, Celery worker, and the Nginx frontend.

    ```bash
    docker-compose up --build -d
    ```

    _Note: The first time you run this, Ollama will download the specified LLM models, which might take some time depending on your internet connection and the model sizes._

4.  **Access the Application:**
    - **Frontend:** Open your web browser and navigate to `http://localhost:8080`
    - **FastAPI Backend:** The API will be available at `http://localhost:8000`

### Usage

1.  **Select a Model:** On the frontend, choose an available LLM. The application will list models that Ollama has loaded. If no models are listed, you may need to pull them manually (see note below).
2.  **Enter a Prompt:** Type your query into the input field.
3.  **Get Inference:** Submit the prompt and observe the asynchronous inference process and the generated response.

#### Note on Ollama Models:

Ollama needs to have models downloaded to perform inferences. You can pull models manually using the Ollama CLI. For example, to pull the `qwen:0.6b` model, run:

```bash
docker exec -it deploy-ai-fastapi-ollama-1 ollama pull qwen:0.6b
```

Replace `deploy-ai-fastapi-ollama-1` with the actual name of your Ollama container if it differs.

#### Example API Request (using `curl`):

You can also interact directly with the FastAPI backend:

```bash
curl -X POST http://localhost:8000/generate \
 -H "x-api-key: dev-key-123" \
 -H "Content-Type: application/json" \
 -d '{"model":"qwen:0.6b","prompt":"Write a haiku about the future of AI."}'
```

This will return a `job_id`:

```json
{ "job_id": "some-unique-job-id", "status": "queued" }
```

You can then poll the status endpoint:

```bash
curl http://localhost:8000/status/some-unique-job-id
```

Once completed, the response will include the generated text:

```json
{ "status": "completed", "result": { "text": "Machines learn and grow,
Future minds begin to bloom,
World transformed anew." } }
```

## üìà Monitoring

Access Prometheus metrics at `http://localhost:8000/metrics`. These metrics provide insights into request latency, request counts, and error rates, allowing for effective monitoring and performance analysis.

## ü§ù Contributing

Contributions are welcome! Please feel free to fork the repository, open issues, or submit pull requests.

## üìÑ License

This project is licensed under the MIT License. See the `LICENSE` file for details.
</file>

<file path="app/services/ollama_client.py">
import httpx
import json
import asyncio
import logging

from app.core.config import settings

logger = logging.getLogger(__name__)

class OllamaClient:
    def __init__(self, base_url: str | None = None):
        """Initialize a self-contained Ollama HTTP client."""
        self.base_url = base_url or settings.OLLAMA_HOST
        # immediately create an AsyncClient ‚Äî no .connect() required
        self.client = httpx.AsyncClient(timeout=120)

    async def list_models(self):
        """Fetch available Ollama models."""
        try:
            r = await self.client.get(f"{self.base_url}/api/tags")
            r.raise_for_status()
            data = r.json()
            return [m["name"] for m in data.get("models", [])]
        except Exception as e:
            logger.error(f"Error fetching Ollama models: {e}")
            return []

    async def generate_once(self, model: str, prompt: str, options: dict = None):
        """Stream inference output and combine into final text."""
        options = options or {}
        payload = {
            "model": model,
            "prompt": prompt,
            "options": options,
            "stream": True
        }

        full_text = ""
        try:
            async with self.client.stream("POST", f"{self.base_url}/api/generate", json=payload) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    try:
                        data = json.loads(line)
                        if "response" in data and data["response"]:
                            full_text += data["response"]
                    except Exception:
                        continue
        except Exception as e:
            logger.error(f"Error during Ollama generation: {e}")
            raise
        return {"model": model, "text": full_text.strip()}

    async def close(self):
        """Cleanly close the HTTP connection."""
        await self.client.aclose()
</file>

<file path="frontend/eslint.config.js">
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import tseslint from 'typescript-eslint'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      js.configs.recommended,
      tseslint.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    rules: {
        'react-refresh/only-export-components': [
            'warn',
            { allowConstantExport: true },
        ],
    },
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
  },
])
</file>

<file path="requirements.txt">
amqp==5.3.1
annotated-types==0.7.0
anyio==4.11.0
billiard==4.2.2
celery==5.5.3
certifi==2025.10.5
charset-normalizer==3.4.4
click==8.3.0
click-didyoumean==0.3.1
click-plugins==1.1.1.2
click-repl==0.3.0
fastapi==0.119.0
filelock==3.20.0
flower==2.0.1
fsspec==2025.9.0
h11==0.16.0
hf-xet==1.1.10
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.35.3
humanize==4.14.0
idna==3.11
iniconfig==2.1.0
jinja2==3.1.6
kombu==5.5.4
markupsafe==3.0.3
mpmath==1.3.0
networkx==3.5
numpy==2.3.3
packaging==25.0
pluggy==1.6.0
prometheus-client==0.23.1
prompt-toolkit==3.0.52
pydantic==2.12.2
pydantic-core==2.41.4
pygments==2.19.2
pytest==8.4.2
pytest-asyncio==1.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.1.1
pytz==2025.2
pyyaml==6.0.3
redis==5.2.1
regex==2025.9.18
requests==2.32.5
respx==0.22.0
safetensors==0.6.2
setuptools==80.9.0
six==1.17.0
sniffio==1.3.1
starlette==0.48.0
sympy==1.14.0
tokenizers==0.22.1
torch==2.8.0
tornado==6.5.2
tqdm==4.67.1
transformers==4.57.1
typing-extensions==4.15.0
typing-inspection==0.4.2
tzdata==2025.2
urllib3==2.5.0
uvicorn==0.37.0
vine==5.1.0
wcwidth==0.2.14
</file>

<file path="app/core/celery_app.py">
from celery import Celery
import os

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

celery_app = Celery(
    "prodify_ai",
    broker=REDIS_URL,
    backend=REDIS_URL,
)

celery_app.conf.update(
    task_track_started=True,
    task_serializer="json",
    result_serializer="json",
    accept_content=["json"],
    timezone="UTC",
    enable_utc=True,
    broker_connection_retry_on_startup=True,
    task_acks_late=True,
    broker_transport_options={"visibility_timeout": 7200},
    task_default_retry_delay=10,
    task_annotations={
        '*': {'max_retries': 3, 'time_limit': 300}
    },
)
</file>

<file path="app/worker.py">
from app.core.celery_app import celery_app
from app.services.ollama_client import OllamaClient
import asyncio
from app.core.logger import logger
import time

@celery_app.task(bind=True, name="generate_text_task")
def generate_text_task(self, model: str, prompt: str, max_tokens: int = 200, temperature: float = 0.7):
    """Celery worker task: independent Ollama connection."""
    async def _run():
        ollama = OllamaClient()
        # self.update_state(state="STARTED", meta={"progress": 0})
        logger.info(f"[{self.request.id}] Starting text generation at {time.time()}")
        result = await ollama.generate_once(
            model=model,
            prompt=prompt,
            options={"num_predict": max_tokens, "temperature": temperature},
        )
        logger.info(f"[{self.request.id}] Finished text generation at {time.time()}")
        await ollama.close()
        return result

    return asyncio.run(_run())
</file>

<file path="frontend/src/App.tsx">
import { useEffect, useState } from "react"
import { Button } from "@/components/ui/button"
import { Textarea } from "@/components/ui/textarea"
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from "@/components/ui/select"
import { Slider } from "@/components/ui/slider"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"
import { Label } from "@/components/ui/label"

function App() {
  const [models, setModels] = useState<string[]>([])
  const [model, setModel] = useState("")
  const [prompt, setPrompt] = useState("Once upon a time in Bangalore,")
  const [maxTokens, setMaxTokens] = useState(200)
  const [loading, setLoading] = useState(false)
  const [output, setOutput] = useState("")
  const [jobId, setJobId] = useState<string | null>(null)
  const [status, setStatus] = useState("idle")

  useEffect(() => {
    fetch("/models")
      .then((res) => res.json())
      .then((data) => {
        setModels(data.models || [])
        if (data.models?.length > 0) setModel(data.models[0])
      })
      .catch((err) => console.error("Failed to load models:", err))
  }, [])

  async function handleGenerate() {
    if (!model || !prompt.trim()) return
    setLoading(true)
    setOutput("")
    setJobId(null)
    setStatus("queued")

    try {
      const res = await fetch("/generate", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "x-api-key": import.meta.env.VITE_API_KEY,
        },
        body: JSON.stringify({ model, prompt, max_tokens: maxTokens }),
      })

      const data = await res.json()
      if (data.job_id) {
        setJobId(data.job_id)
        pollStatus(data.job_id)
      } else {
        setOutput("Unexpected response: " + JSON.stringify(data))
        setLoading(false)
      }
    } catch (err) {
      console.error(err)
      setOutput("Error generating text.")
      setLoading(false)
    }
  }

  async function pollStatus(jobId: string) {
    const interval = setInterval(async () => {
      const res = await fetch(`/status/${jobId}`)
      const data = await res.json()

      if (data.status === "completed") {
        clearInterval(interval)
        setOutput(data.result.text || "No text returned.")
        setStatus("completed")
        setLoading(false)
      } else if (data.status === "failed") {
        clearInterval(interval)
        setOutput(`‚ùå Failed: ${data.error}`)
        setStatus("failed")
        setLoading(false)
      } else {
        setStatus(data.status)
      }
    }, 2000)
  }

  return (
    <div className="min-h-screen bg-gray-50 flex flex-col items-center py-10 px-4">
      <div className="w-full max-w-2xl space-y-6">
        <h1 className="text-3xl font-bold text-center">üöÄ Prodify Text Inference Queue</h1>
        <p className="text-center text-gray-600">
          Generate text asynchronously via FastAPI + Celery + Ollama.
        </p>

        {/* Model Selector */}
        <div className="space-y-2">
          <Label>Select Model</Label>
          <Select value={model} onValueChange={setModel}>
            <SelectTrigger>
              <SelectValue placeholder="Choose a model..." />
            </SelectTrigger>
            <SelectContent>
              {models.map((m) => (
                <SelectItem key={m} value={m}>
                  {m}
                </SelectItem>
              ))}
            </SelectContent>
          </Select>
        </div>

        {/* Prompt Input */}
        <div className="space-y-2">
          <Label>Prompt</Label>
          <Textarea
            value={prompt}
            onChange={(e) => setPrompt(e.target.value)}
            placeholder="Enter your text prompt..."
            rows={5}
          />
        </div>

        {/* Token Slider */}
        <div className="space-y-2">
          <Label>Max Tokens: {maxTokens}</Label>
          <Slider
            value={[maxTokens]}
            onValueChange={(val) => setMaxTokens(val[0])}
            min={10}
            max={500}
            step={10}
          />
        </div>

        {/* Generate Button */}
        <div className="flex justify-center">
          <Button onClick={handleGenerate} disabled={loading || !model}>
            {loading ? `Generating (${status})...` : "Generate"}
          </Button>
        </div>

        {/* Output Card */}
        <Card className="mt-6">
          <CardHeader>
            <CardTitle>Output</CardTitle>
          </CardHeader>
          <CardContent>
            {jobId && (
              <div className="text-xs text-gray-500 mb-2">
                Job ID: <code>{jobId}</code> ({status})
              </div>
            )}
            <pre className="whitespace-pre-wrap text-sm">
              {output || "No output yet."}
            </pre>
          </CardContent>
        </Card>
      </div>
    </div>
  )
}

export default App
</file>

<file path="docker-compose.yml">
services:
  # Redis service for Celery message broker and result backend
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # Ollama service for LLM inference
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 5s
      timeout: 10s
      retries: 5
    environment:
      - OLLAMA_LOAD_TIMEOUT=600

  # Backend FastAPI server
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    cache:
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-backend:latest
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-backend:main,mode=max
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - API_KEYS=${API_KEYS}
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy

  # Celery worker for background tasks
  worker:
    build:
      context: .
      dockerfile: Dockerfile
    cache:
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-frontend:latest
        - type=registry,ref=ghcr.io/${GITHUB_REPOSITORY_OWNER}/prodify-cache-frontend:main,mode=max
    command: celery -A app.worker.celery_app worker --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      redis:
        condition: service_started
      ollama:
        condition: service_healthy

  # Frontend Nginx server
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.frontend
    ports:
      - "8080:80"
    depends_on:
      backend:
        condition: service_started

volumes:
  ollama_data:
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  test-backend:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run tests
        run: PYTHONPATH=. pytest

  test-frontend:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: ./frontend
        run: npm install

      - name: Lint frontend
        working-directory: ./frontend
        run: npm run lint

      - name: Build frontend
        working-directory: ./frontend
        run: npm run build

  integration-test:
    runs-on: ubuntu-latest
    needs: [test-backend, test-frontend]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ‚¨áÔ∏è STEP 1: ADDED - Log in to GHCR to store/retrieve cache layers
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # ‚¨áÔ∏è STEP 2: ADDED - Set up the advanced builder required for caching
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3


      - name: Build and run Docker Compose
        env:
          API_KEYS: dev-key-123
        run: docker compose up -d --build

      - name: Install jq
        run: sudo apt-get install -y jq

      - name: Wait for services to be healthy
        run: |
          echo "Waiting for backend..."
          while [[ "$(curl -s -o /dev/null -w '%{http_code}' http://localhost:8000/healthz)" != "200" ]]; do
            sleep 2
          done
          echo "Waiting for frontend..."
          while [[ "$(curl -s -o /dev/null -w '%{http_code}' http://localhost:8080/)" != "200" ]]; do
            sleep 2
          done
          echo "Services are up!"

      - name: Run End-to-End Test
        env:
          API_KEYS: dev-key-123
        run: |
          # (Your test script remains the same)
          echo "Starting end-to-end test for /generate endpoint..."

          HTTP_RESPONSE=$(curl -s -w "\n%{http_code}" -X POST -H "Content-Type: application/json" -H "x-api-key: $API_KEYS" -d '{"model":"qwen3:0.6b","prompt":"a haiku about CI/CD"}' http://localhost:8080/generate)
          HTTP_STATUS=$(echo "$HTTP_RESPONSE" | tail -n1)
          HTTP_BODY=$(echo "$HTTP_RESPONSE" | sed '$d')

          echo "Response Body: $HTTP_BODY"
          echo "Response Status: $HTTP_STATUS"

          if [ "$HTTP_STATUS" -ne 200 ]; then
            echo "/generate endpoint failed with status $HTTP_STATUS"
            exit 1
          fi

          JOB_ID=$(echo $HTTP_BODY | jq -r '.job_id')

          if [ -z "$JOB_ID" ] || [ "$JOB_ID" == "null" ]; then
            echo "Failed to get a valid job_id from response!"
            exit 1
          fi
          echo "Job submitted with ID: $JOB_ID"

          echo "Polling for job completion..."
          for i in {1..60}; do
            STATUS_RESPONSE=$(curl -s http://localhost:8080/status/$JOB_ID)
            JOB_STATUS=$(echo $STATUS_RESPONSE | jq -r '.status')
            echo "Current job status: $JOB_STATUS"

            if [ "$JOB_STATUS" == "completed" ]; then
              echo "Job completed successfully!"
              RESULT_TEXT=$(echo $STATUS_RESPONSE | jq -r '.result.text')
              echo "Result: $RESULT_TEXT"
              exit 0
            elif [ "$JOB_STATUS" == "failed" ]; then
              echo "Job failed!"
              echo "Response: $STATUS_RESPONSE"
              exit 1
            fi
            sleep 5
          done

          echo "Job did not complete in time!"
          exit 1

      - name: View logs on failure
        if: failure()
        run: docker compose logs

      - name: Tear down services
        if: always()
        run: docker compose down
</file>

</files>
